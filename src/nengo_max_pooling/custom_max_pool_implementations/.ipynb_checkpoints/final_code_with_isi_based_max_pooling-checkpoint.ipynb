{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "injured-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nengo_dl\n",
    "import nengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "common-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedicated-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-sight",
   "metadata": {},
   "source": [
    "# TF Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "permanent-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "inp = tf.keras.Input(shape=(1, 28, 28)) # Channels first.\n",
    "\n",
    "# convolutional layers\n",
    "conv0 = tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation=tf.nn.relu,\n",
    "    data_format=\"channels_first\"\n",
    ")(inp)\n",
    "\n",
    "# Default pool_size = (2,2), padding = \"valid\", data_format = \"channels_last\".\n",
    "max_pool = tf.keras.layers.MaxPool2D(data_format = \"channels_first\")(conv0) \n",
    "\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    strides=2,\n",
    "    activation=tf.nn.relu,\n",
    "    data_format=\"channels_first\"\n",
    ")(max_pool)\n",
    "\n",
    "# fully connected layer\n",
    "flatten = tf.keras.layers.Flatten()(conv1)\n",
    "dense = tf.keras.layers.Dense(units=10, activation=\"softmax\")(flatten)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "listed-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 28, 28)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 13, 13)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 6, 6)          18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                23050     \n",
      "=================================================================\n",
      "Total params: 41,866\n",
      "Trainable params: 41,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-stylus",
   "metadata": {},
   "source": [
    "# TF Model Compilation and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worldwide-amazon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Channels first coding.\n",
    "train_images = train_images.reshape([train_images.shape[0], 1] + list(train_images.shape[1:])) # (60000, 1, 28, 28)\n",
    "test_images = test_images.reshape([test_images.shape[0], 1] + list(test_images.shape[1:]))\n",
    "\n",
    "print(train_images.shape, test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cloudy-sociology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2714 - sparse_categorical_accuracy: 0.9427\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9790\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0583 - sparse_categorical_accuracy: 0.9825\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0481 - sparse_categorical_accuracy: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ae8def25c90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.optimizers.Adam(0.001),\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=[tf.metrics.sparse_categorical_accuracy])\n",
    "model.fit(train_images, train_labels, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-housing",
   "metadata": {},
   "source": [
    "# TF Model evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "experienced-zoning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0872 - sparse_categorical_accuracy: 0.9739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08719835430383682, 0.9739000201225281]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-breach",
   "metadata": {},
   "source": [
    "# Conversion from TF to spiking Nengo DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "given-policy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Cannot convert max pooling layers to native Nengo objects; consider setting max_to_avg_pool=True to use average pooling instead. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:588: UserWarning: Activation type <function softmax at 0x2ae8b9bdcc20> does not have a native Nengo equivalent; falling back to a TensorNode\n",
      "  \"falling back to a TensorNode\" % activation\n"
     ]
    }
   ],
   "source": [
    "n_steps = 40\n",
    "np.random.seed(100)\n",
    "ndl_model = nengo_dl.Converter(model, \n",
    "                               swap_activations={tf.nn.relu: nengo.SpikingRectifiedLinear()},\n",
    "                               scale_firing_rates=100,\n",
    "                               synapse=0.005)\n",
    "\n",
    "with ndl_model.net:\n",
    "  nengo_dl.configure_settings(stateful=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-weather",
   "metadata": {},
   "source": [
    "# Nengo-DL model test data creation and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handled-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndl_test_images = np.tile(\n",
    "  test_images.reshape((test_images.shape[0], 1, -1)), (1, n_steps, 1))\n",
    "ndl_input = ndl_model.inputs[inp]\n",
    "ndl_output = ndl_model.outputs[dense]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sustained-toilet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Constructing graph: build stage finished in 0:00:00                            \r"
     ]
    }
   ],
   "source": [
    "with nengo_dl.Simulator(\n",
    "  ndl_model.net, minibatch_size=100) as sim:\n",
    "  data1 = sim.predict({ndl_input: ndl_test_images[:200]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-twins",
   "metadata": {},
   "source": [
    "# Nengo-DL model accuracy (first 200 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "independent-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for pred, true in zip(data1[ndl_output][:, -1, :], test_labels):\n",
    "  if np.argmax(pred) == true:\n",
    "    acc += 1\n",
    "print(acc/200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-resolution",
   "metadata": {},
   "source": [
    "# ###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-cleaner",
   "metadata": {},
   "source": [
    "# Nengo-DL model modification (Replacing MaxPooling TensorNode with Custom Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ambient-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x = 4\n",
    "NEURONS_LAST_SPIKED_TS, NEURONS_LATEST_ISI = np.zeros((26, 26, 32)), np.ones((26, 26, 32))*np.inf\n",
    "MAX_POOL_MASK = np.ones((26, 26, 32))/num_x\n",
    "\n",
    "def isi_based_max_pooling(t, inp):\n",
    "  # Reshape `inp` of shape (21632,) to matrix of shape (26, 26, 32). This operation should preserve the input\n",
    "  # topography from the first Conv layer.\n",
    "  # inp = inp.reshape(26, 26, 32) # Channels last.\n",
    "  inp = inp.reshape(32, 26, 26) # Channels first.\n",
    "  # Now the output with 2 x 2 MaxPooling should be of shape (13, 13, 32) and then flattened.\n",
    "  # ret = np.zeros((13, 13, 32)) # Channels last.\n",
    "  ret = np.zeros((32, 13, 13)) # Channels first.\n",
    "  \n",
    "  ##################### Normal MaxPooling with np.max() function. #####################\n",
    "  #for chnl in range(inp.shape[2]): # For each channel, calculate the MaxPooled values.\n",
    "  for chnl in range(inp.shape[0]): # For each channel, calculate the MaxPooled values.\n",
    "    for r in range(13):\n",
    "      for c in range(13):\n",
    "        ret[chnl, r, c] = np.max(inp[chnl, r*2:r*2+2, c*2:c*2+2]) # Channels first\n",
    "        #ret[r, c, chnl] = np.max(inp[r*2:r*2+2, c*2:c*2+2, chnl]) # Channels last\n",
    "  #####################################################################################\n",
    "  \n",
    "  \n",
    "  ##################### MaxPooling with ISI based method. ##############################\n",
    "  \n",
    "#   def _isi_max_pool_algorithm(t, x, r1, r2, c1, c2, chnl):\n",
    "#     int_t = int(t*1000.0)\n",
    "#     # Get the local copies of updated NEURONS_LAST_SPIKED_TS, NEURONS_LATEST_ISI, and MAX_POOL_MASK for\n",
    "#     # each timestep.\n",
    "#     neurons_last_spiked_ts = NEURONS_LAST_SPIKED_TS[r1:r2, c1:c2, chnl].flatten()\n",
    "#     neurons_latest_isi = NEURONS_LATEST_ISI[r1:r2, c1:c2, chnl].flatten()\n",
    "#     max_pool_mask = MAX_POOL_MASK[r1:r2, c1:c2, chnl].flatten()\n",
    "    \n",
    "#     spiked_neurons_mask = np.logical_not(np.isin(x, 0))\n",
    "#     if np.all(spiked_neurons_mask == False):\n",
    "#       return 0\n",
    "    \n",
    "#     if np.any(neurons_last_spiked_ts[spiked_neurons_mask]):\n",
    "#       neurons_last_spiked_ts_mask = np.logical_not(np.isin(neurons_last_spiked_ts, 0))\n",
    "#       neurons_isi_to_be_updated = neurons_last_spiked_ts_mask & spiked_neurons_mask\n",
    "#       neurons_latest_isi[neurons_isi_to_be_updated] = int_t - neurons_last_spiked_ts[neurons_isi_to_be_updated]\n",
    "#       max_pool_mask[:] = np.zeros(num_x)\n",
    "#       max_pool_mask[np.argmin(neurons_latest_isi)] = 1.0\n",
    "#       neurons_last_spiked_ts[spiked_neurons_mask] = int_t\n",
    "      \n",
    "#       NEURONS_LAST_SPIKED_TS[r1:r2, c1:c2, chnl] = neurons_last_spiked_ts.reshape(2, 2)\n",
    "#       NEURONS_LATEST_ISI[r1:r2, c1:c2, chnl] = neurons_latest_isi.reshape(2, 2)\n",
    "#       MAX_POOL_MASK[r1:r2, c1:c2, chnl] = max_pool_mask.reshape(2, 2)\n",
    "      \n",
    "#       return np.dot(max_pool_mask, x)\n",
    "    \n",
    "#     else:\n",
    "#       if np.min(neurons_latest_isi) != np.inf:\n",
    "#         max_pool_mask[:] = np.zeros(num_x)\n",
    "#         max_pool_mask[np.argmin(neurons_latest_isi)] = 1.0\n",
    "#         neurons_last_spiked_ts[spiked_neurons_mask] = int_t\n",
    "#       else:\n",
    "#         if np.any(neurons_last_spiked_ts):\n",
    "#           neurons_last_spiked_ts_mask = np.logical_not(np.isin(neurons_last_spiked_ts, 0))\n",
    "#           minimum_last_spiked_ts = np.min(neurons_last_spiked_ts[neurons_last_spiked_ts_mask])\n",
    "#           first_spike_neuron_index = np.where(neurons_last_spiked_ts == minimum_last_spiked_ts)\n",
    "#           max_pool_mask[:] = np.zeros(num_x)\n",
    "#           max_pool_mask[first_spike_neuron_index] = 1.0\n",
    "#           neurons_last_spiked_ts[spiked_neurons_mask] = int_t\n",
    "#         else:\n",
    "#           neurons_last_spiked_ts[spiked_neurons_mask] = int_t\n",
    "#           max_pool_mask[:] = np.zeros(num_x)\n",
    "#           max_pool_mask[np.where(neurons_last_spiked_ts)[0]] = 1.0\n",
    "       \n",
    "#       NEURONS_LAST_SPIKED_TS[r1:r2, c1:c2, chnl] = neurons_last_spiked_ts.reshape(2, 2)\n",
    "#       NEURONS_LATEST_ISI[r1:r2, c1:c2, chnl] = neurons_latest_isi.reshape(2, 2)\n",
    "#       MAX_POOL_MASK[r1:r2, c1:c2, chnl] = max_pool_mask.reshape(2, 2)\n",
    "#       return np.dot(max_pool_mask, x)\n",
    "    \n",
    "#   for chnl in range(inp.shape[2]):\n",
    "#     for r in range(13):\n",
    "#       for c in range(13):\n",
    "#         ret[r, c, chnl] = _isi_max_pool_algorithm(\n",
    "#             t, inp[r*2:r*2+2, c*2:c*2+2, chnl].flatten(), r*2, r*2+2, c*2, c*2+2, chnl)\n",
    "  \n",
    "  return ret.flatten()\n",
    "  #return x[:5408]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacterial-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ndl_model.net:\n",
    "  # Create Custom Node.\n",
    "  new_node = nengo.Node(output=isi_based_max_pooling, size_in=21632, label=\"Custom Node\")\n",
    "\n",
    "  conn_from_conv0_to_max_node = ndl_model.net.all_connections[3]\n",
    "\n",
    "  # COnnection from Conv0 to MaxPool node.\n",
    "  nengo.Connection(\n",
    "    conn_from_conv0_to_max_node.pre_obj,\n",
    "    new_node,\n",
    "    transform=conn_from_conv0_to_max_node.transform,\n",
    "    synapse=conn_from_conv0_to_max_node.synapse,\n",
    "    function=conn_from_conv0_to_max_node.function)\n",
    "\n",
    "  # Connection from MaxPool node to Conv1.\n",
    "  conn_from_max_node_to_conv1 = ndl_model.net.all_connections[6]\n",
    "  nengo.Connection(\n",
    "    new_node,\n",
    "    conn_from_max_node_to_conv1.post_obj,\n",
    "    transform=conn_from_max_node_to_conv1.transform,\n",
    "    synapse=conn_from_max_node_to_conv1.synapse,\n",
    "    function=conn_from_max_node_to_conv1.function)\n",
    "\n",
    "  # Remove the old connection to MaxPool node and from MaxPool node, MaxPool node.\n",
    "  ndl_model.net._connections.remove(conn_from_conv0_to_max_node)\n",
    "  ndl_model.net._connections.remove(conn_from_max_node_to_conv1)\n",
    "  ndl_model.net._nodes.remove(conn_from_conv0_to_max_node.post_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-difference",
   "metadata": {},
   "source": [
    "# Check if modification of connection was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "driven-wyoming",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Connection at 0x2ae83fa09a10 from <Node \"conv2d.0.bias\"> to <Node \"conv2d.0.bias_relay\">>,\n",
       " <Connection at 0x2ae8dfd6f050 from <Node \"conv2d.0.bias_relay\"> to <Neurons of <Ensemble \"conv2d.0\">>>,\n",
       " <Connection at 0x2ae8dfd7aa10 from <Node \"input_1\"> to <Neurons of <Ensemble \"conv2d.0\">>>,\n",
       " <Connection at 0x2ae8dfd36ed0 from <Node \"conv2d_1.0.bias\"> to <Node \"conv2d_1.0.bias_relay\">>,\n",
       " <Connection at 0x2ae8dfd36a10 from <Node \"conv2d_1.0.bias_relay\"> to <Neurons of <Ensemble \"conv2d_1.0\">>>,\n",
       " <Connection at 0x2ae8dfd36150 from <Node \"dense.0.bias\"> to <TensorNode \"dense.0\">>,\n",
       " <Connection at 0x2ae8dfd363d0 from <Neurons of <Ensemble \"conv2d_1.0\">> to <TensorNode \"dense.0\">>,\n",
       " <Connection at 0x2aec6e25e550 from <Neurons of <Ensemble \"conv2d.0\">> to <Node \"Custom Node\">>,\n",
       " <Connection at 0x2ae8dfd36910 from <Node \"Custom Node\"> to <Neurons of <Ensemble \"conv2d_1.0\">>>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndl_model.net.all_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opposed-yugoslavia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Constructing graph: build stage finished in 0:00:00                            \r"
     ]
    }
   ],
   "source": [
    "with nengo_dl.Simulator(\n",
    "  ndl_model.net, minibatch_size=100) as sim:\n",
    "  data2 = sim.predict({ndl_input: ndl_test_images[:200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "successful-defendant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for pred, true in zip(data2[ndl_output][:, -1, :], test_labels):\n",
    "  if np.argmax(pred) == true:\n",
    "    acc += 1\n",
    "print(acc/200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
