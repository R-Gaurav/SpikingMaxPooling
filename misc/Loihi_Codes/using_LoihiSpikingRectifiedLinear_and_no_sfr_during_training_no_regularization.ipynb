{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "environmental-sacrifice",
   "metadata": {},
   "source": [
    "### With no `scale_firing_rates` during training, but with `LoihiSpikingRectifiedLinear()` and  no regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-rebel",
   "metadata": {},
   "source": [
    "Do not set `amplitude` and `max_rates` as you are not creating `Ensembles` from scratch, rather, converting a trained TF Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "theoretical-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nengo.utils.matplotlib import rasterplot\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-trigger",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "  tf.keras.datasets.mnist.load_data()\n",
    ")\n",
    "# Make it channels_first.\n",
    "train_images, test_images = np.expand_dims(train_images, -1), np.expand_dims(test_images, -1)\n",
    "train_images, test_images = np.moveaxis(train_images, -1, 1), np.moveaxis(test_images, -1, 1)\n",
    "\n",
    "# Flatten images and add time dimension.\n",
    "train_images = train_images.reshape((train_images.shape[0], 1, -1))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1, -1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 1, -1))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], 1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-numbers",
   "metadata": {},
   "source": [
    "# Design the TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "found-artwork",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1, 28, 28)]       0         \n",
      "_________________________________________________________________\n",
      "to-spikes (Conv2D)           (None, 3, 28, 28)         3         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 8, 26, 26)         216       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 16, 12, 12)        1152      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense0 (Dense)               (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 149,541\n",
      "Trainable params: 149,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input(shape=(1, 28, 28), name=\"input\")\n",
    "\n",
    "to_spikes = tf.keras.layers.Conv2D(\n",
    "  filters=3, # 3 RGB Neurons per pixel.\n",
    "  kernel_size=(1, 1), strides=(1, 1), activation=tf.nn.relu, use_bias=False, # Default is True.\n",
    "  data_format=\"channels_first\", name=\"to-spikes\")(inp)\n",
    "\n",
    "conv0 = tf.keras.layers.Conv2D(\n",
    "  filters=8, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv0\")(to_spikes)\n",
    "\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "  filters=16, kernel_size=(3, 3), strides=(2, 2), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv1\")(conv0)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten(name=\"flatten\")(conv1)\n",
    "dense0 = tf.keras.layers.Dense(64, activation=tf.nn.relu, name=\"dense0\")(flatten)\n",
    "\n",
    "#dense1 = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"dense1\")(dense0) # Results in formation of TensorNode\n",
    "                                                                                # which isn't supported on Loihi.\n",
    "dense1 = tf.keras.layers.Dense(10, name=\"dense1\")(dense0)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=dense1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thirty-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nengo_dl_model(**kwargs):\n",
    "  converter = nengo_dl.Converter(model, **kwargs)\n",
    "  lyr_probes = []\n",
    "  with converter.net:\n",
    "    probe_conv0 = nengo.Probe(converter.layers[conv0])\n",
    "    probe_conv1 = nengo.Probe(converter.layers[conv1])\n",
    "    probe_dense0 = nengo.Probe(converter.layers[dense0])\n",
    "    lyr_probes.extend([probe_conv0, probe_conv1, probe_dense0])\n",
    "  return converter, lyr_probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-belgium",
   "metadata": {},
   "source": [
    "# Train the model and save the Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "choice-frame",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/4\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.2084 - probe_loss: 0.2084 - probe_accuracy: 0.9379\n",
      "Epoch 2/4\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.0595 - probe_loss: 0.0595 - probe_accuracy: 0.9816\n",
      "Epoch 3/4\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.0365 - probe_loss: 0.0365 - probe_accuracy: 0.9889\n",
      "Epoch 4/4\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.0233 - probe_loss: 0.0233 - probe_accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "params_file, epochs = \"./attempting_TN_MP_loihineurons_8_16\", 4\n",
    "\n",
    "converter, lyr_probes = _get_nengo_dl_model(\n",
    "  #swap_activations={tf.nn.relu: nengo_loihi.neurons.SpikingRectifiedLinear()}, # With this neuron, spike amplitude\n",
    "                                                                                # is more than 1.\n",
    "  swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}, # Spike amplitude is just 1.\n",
    ")\n",
    "  \n",
    "with nengo_dl.Simulator(converter.net, seed=0, minibatch_size=200) as sim:\n",
    "    losses = {\n",
    "      converter.outputs[dense1]: tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      converter.outputs[dense1]: \"accuracy\"\n",
    "    }\n",
    "      \n",
    "    sim.compile(\n",
    "      loss=losses,\n",
    "      optimizer=tf.optimizers.Adam(0.001),\n",
    "      #metrics={converter.outputs[dense1]: tf.metrics.sparse_categorical_accuracy},\n",
    "      metrics=metrics,\n",
    "    )\n",
    "    sim.fit(\n",
    "      {converter.inputs[inp]: train_images},\n",
    "      {converter.outputs[dense1]: train_labels},\n",
    "      epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # save the parameters to file.\n",
    "    sim.save_params(params_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-richards",
   "metadata": {},
   "source": [
    "# Print the `gain`, `bias`, `intercepts`, and `max_rates` of the `Dense0` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clear-hands",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/simulator.py:2266: UserWarning: Checking parameters after simulator is closed; cannot fetch live values, so the initial values will be returned.\n",
      "  \"Checking parameters after simulator is closed; \"\n"
     ]
    }
   ],
   "source": [
    "print(sim.data[converter.layers[dense0].ensemble].gain)\n",
    "print(sim.data[converter.layers[dense0].ensemble].bias)\n",
    "print(sim.data[converter.layers[dense0].ensemble].intercepts)\n",
    "print(sim.data[converter.layers[dense0].ensemble].max_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "# Test the NengoDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nengo DL Test Acc: 97.0\n"
     ]
    }
   ],
   "source": [
    "n_steps=30\n",
    "scale_firing_rates=100\n",
    "synapse=0.005\n",
    "n_test=100\n",
    "sfr=100\n",
    "\n",
    "nengo_converter, layer_probes = _get_nengo_dl_model(\n",
    "    scale_firing_rates=sfr, \n",
    "    #swap_activations={tf.nn.relu: nengo_loihi.neurons.SpikingRectifiedLinear()}, # With this neuron, spike \n",
    "                                                                                  # amplitude is more than 1.\n",
    "    swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}, # Spike Amplitude is just 1.\n",
    "    synapse=synapse\n",
    ")\n",
    "  \n",
    "tiled_test_images = np.tile(test_images[:n_test], (1, n_steps, 1))\n",
    "# Speed up simulation.\n",
    "with nengo_converter.net:\n",
    "  nengo_dl.configure_settings(stateful=False)\n",
    "    \n",
    "# Build network, load in trained weights, do inference.\n",
    "with nengo_dl.Simulator(\n",
    "    nengo_converter.net, minibatch_size=20, progress_bar=False) as sim:\n",
    "  sim.load_params(params_file)\n",
    "  data = sim.predict({nengo_converter.inputs[inp]: tiled_test_images})\n",
    "    \n",
    "test_predictions = np.argmax(data[nengo_converter.outputs[dense1]][:, -1], axis=-1)\n",
    "print(\"Nengo DL Test Acc: %s\" % (100 * np.mean(test_predictions == test_labels[:n_test, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-supervisor",
   "metadata": {},
   "source": [
    "# Plotting the spikes in Conv0 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominant-nightmare",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6UAAAHSCAYAAAAUmW0WAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgDklEQVR4nO3de6xl91Uf8LXw5IFC/AieOn7VYxMDQjxCdOSmgKpcglGSljqtSpSIFhdZTSuSKEBFSfgHWrUKNC0m2HLowxFOZR4xj9pGUZVRuBWtDDHnEpOEBJJhksF2JrHJY0yESjFZ/WO2q0nmjj2euWevM+f3+Uije87eZ3zXb/1+Z0++2efsnVUVAAAA0OErugsAAABgXEIpAAAAbYRSAAAA2gilAAAAtBFKAQAAaCOUAgAA0GZfdwERERdffHEdOHCguwwAAABWYGdn58+qav9u+9YilB44cCCWy2V3GQAAAKxAZh451T4f3wUAAKCNUApsnEdvubW7BFgpa5xNZ42zyazvkwmlAAAAtMmq6q4hFotF+U4pAADAZsrMnapa7LbPmVIAAADaCKUAAAC0EUoBAABoI5QCAADQRigFAACgjVAKAABAG6EUAACANkIpAAAAbYRSmMGxg0e6SxiKfs9Pz+el3/PS7/np+bz0e176fTKhFAAAgDZZVd01xGKxqOVy2V0GAAAAK5CZO1W12G2fM6UAAAC0EUoBAABoI5QCAADQRigFAACgjVAKAABAG6EUAACANkIpAAAAbYRSAAAA2gilAAAAtBFKAQAAaCOUAgAA0EYoBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG2EUgAAANoIpQAAALQRShnGWz9+tLuEoej3/PR8Xvo9L/2en57PS7/np+fz+or9l1x2yn1zFgIAAAAnyqrqriEWi0Utl8vuMgAAAFiBzNypqsVu+5wpZRiP3nJrdwlD0e/56fm89Hte+j0/PZ+Xfs9Pz+f1/H37fHwXAACA9ePjuwAAAKyUj+8CAACwloRSAAAA2gilAAAAtBFKAQAAaCOUAgAA0EYoBQAAoI1QCgCwpu6/93B3CQArJ5QCAADQRigFVuLYwSPdJTAA64w5dK6z677nmrbfzbwcz5jDuq4zoRQAAIA2WVXdNcRisajlctldBgAAACuQmTtVtdhtnzOlAAAAtBFKAQAAaCOUAgAA0EYoBQAAoI1QCjNw8/N56ff89Hxe+j0v/Z6fns9Lv+el3ycTSgEAAGjjljAAAACslFvCAAAAsJaEUgAAANoIpQAAALQRSgEAAGgjlAIAANBGKAUAAKCNUArAOeu+u+7sLgHYII4p0EMoBQAAoE1WVXcNsVgsarlcdpcBAADACmTmTlUtdtvnTCkAAABthFIA4LS99eNHu0sA4Ayt6zFcKAUAAKCNUAoAnLYfvfrS7hIAOEPregwXSgEAAGgjlAIAANBGKAUAAKCNUAoAAEAboRQAAIA2QikAAABthFIAABjYzQc/2l0CgxNKAQAAaCOUAgDAwH74+q/tLoHBCaUAAAC0ecpQmpnvyMxHMvNDJ2x7XmYezMyPTT8vmrZnZv5cZh7KzA9k5otWWTwAAADnttM5U/oLEfGyL9v2poh4b1VdGxHvnZ5HRLw8Iq6d/rw2It6+N2UCAACwiZ4ylFbVb0fEZ79s8w0Rccf0+I6IeOUJ299Zx/1uRFyYmZfuUa0Aa297e7u7BFgpa5xNZn2z6dZ1jZ/pd0ovqaqj0+NPRcQl0+PLI+LBE1730LQNAAAATnLWFzqqqoqI2oNaAM55W1tb3SXASlnjbDLrm023rmv8TEPpp5/4WO7085Fp+8MRceUJr7ti2gYAAAAnOdNQek9E3Dg9vjEi7j5h+/dPV+F9cUQcO+FjvgAAAPAl9j3VCzLzlyLiJRFxcWY+FBE/ERE/FRHvysybIuJIRLxqevm7I+IVEXEoIv4iIn5gBTUDAACwIU7n6ruvqapLq+oZVXVFVd1eVZ+pqpdW1bVV9V1V9dnptVVVr6uqr6mqb6qq5eqHADyZ++893F0CMzHX4zDX89Lv+en5vPR7Xvp9srO+0BEAAACcqTx+8dxei8WilksnVQEAADZRZu5U1WK3fc6UAgAA0EYoBQAAoI1QCgAAQBuhFAA4fdtv6a4AgDO1psdwoRQAAIA2QikAcPq23txdAQBnak2P4UIpAAAAbYRSAAAA2gilAHtoe3u7uwRYKWucTWZ9s+nWdY0LpQAAALTJququIRaLRS2Xy+4yAAAAWIHM3KmqxW77nCllHGt6X6aNpd/z0/N5jdrvEcc94pgjxhz3iGOOMO6RrOmYhVIAAADa+PguAAAAK+XjuwAAAKwloRQAAIA2QikAAABthFIAAADaCKUAAAC0EUoBAABoI5QCAADQRihlVo/ecmt3CcMZsecjjjlizHGPOOaIMcc94pgjjHskI445YsxxjzjmiIjn79t32an2CaUAAAC0yarqriEWi0Utl8vuMgAAAFiBzNypqsVu+5wpBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG2EUgAAANoIpQAAALQRShnGqDcq7qLf89Pzeen3vEbtt3GPY8QxR4w57hHHHBHx/H37LjvVPqEUAACANkIpw/jY8y/qLmEoo/b7vrvubPvd+9/w+rbf3TnuLtb4vEZd38Y9rxHHHGHccxtxzBERn3r88U+eap9QCgAAQJusqu4aYrFY1HK57C4DAACAFcjMnapa7LbPmVIAAADaCKUAAAC0EUoBAABoI5QCAADQRihlGPffe7i7hKHo9/xG7PmIY44Yc9wjjjnCuEcy4pgjxhz3iGOOiLjwOfsvO9U+oRQAAIA2bgkDAADASrklDAAAAGtJKGVWb/340e4ShjNiz0ccc8SY4x5xzBFjjnvEMUcY90hGHHPEmOMeccwREV+x/xLfKQUAAGD9+E4pAAAAK+U7pQAAAKwloRQAAIA2QikAAABthFJgJY4dPNJdAqyUNc4crDPmYJ0xh0ufu9/Vd2FU29vb3SUwE3M9DnM9jt/50/d3lzAU762xmO/1IZQCK3HB9Vd1lwArZY0zh2ddc0F3CQzA8Yw5HP3zRz95qn1uCQMAAMBKuSUMAAAAa0koBQAAoI1QCgDASR695dbuEoBBCKUAAAC0EUoBADjJ/je8vrsEYBBCKQAAAG2EUgAAANoIpQAAALQRShnGzQc/2l3CUPR7fno+r1H7PeK4RxxzxJjjHnHMEcY9knUds1AKAABAm6yq7hpisVjUcrnsLgMAAIAVyMydqlrsts+ZUgAAANoIpQAAALQRSgEAAGgjlAIAa297e7u7BADOwvnnn3/ZqfYJpQAAALQRSgGAtbe1tdVdAgBn4bHHHvvkqfYJpQAAALQRShnGbQ/c1l3CUG77jdd0lzCcEdf4qOtsyLnuHPP2W9p+9YhzHdE4bnM9uxHf26PO9b7n7fOdUgAAANZPVlV3DbFYLGq5XHaXAQAAwApk5k5VLXbb95RnSjPzyszczswPZ+YfZuYbp+3Py8yDmfmx6edF0/bMzJ/LzEOZ+YHMfNHeDgcAAIBNcTof3308Iv5lVX1DRLw4Il6Xmd8QEW+KiPdW1bUR8d7peUTEyyPi2unPayPi7XteNQAAABvhKUNpVR2tqt+fHv95RHwkIi6PiBsi4o7pZXdExCunxzdExDvruN+NiAsz89K9LhwAGMf29nZ3CQCsyNO60FFmHoiIb42I90XEJVV1dNr1qYi4ZHp8eUQ8eMJfe2jaBgAAAF/itENpZn5VRPxaRPxQVT124r46frWk/ismAQAbaWtrq7sEAFbktEJpZj4jjgfSO6vq16fNn37iY7nTz0em7Q9HxJUn/PUrpm0AAOeexntXAozgdK6+mxFxe0R8pKp+5oRd90TEjdPjGyPi7hO2f/90Fd4XR8SxEz7mCwAAAP/fvtN4zbdHxD+JiA9m5gPTth+PiJ+KiHdl5k0RcSQiXjXte3dEvCIiDkXEX0TED+xlwQAAs9p6c3cFABvtdK6++7+rKqvqm6vqhdOfd1fVZ6rqpVV1bVV9V1V9dnp9VdXrquprquqbqmq5+mEA0Om+u+7sLgHYY97XsHnW9X39tK6+CwAAAHspj184t9disajl0glVAACATZSZO1W12G2fM6UAAAC0EUoBAABoI5QCAADQRigFAACgjVAKAABAG6EUAACANkIpAAAAbYRSAAAA2gilAAAAtBFKAQAAaCOUAgAA0EYoBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG2EUgAAANoIpcBKbG9vd5fAAKyzcZjrsZjvcZjrcZx//vmXnWqfUAoAAEAboRQ23Fs/frTl925tbbX8XnpYZ6za8sDXd5fAjMz3OMz1OL7wrK885T6hFAAAgDZZVd01xGKxqOVy2V0GAAAAK5CZO1W12G2fM6UAAAC0EUoBAABoI5QCAADQRiiFGRw7eKS7BGZiruel3/PT83np97z0e356Pq917bdQCgAAQBuhFGZwwfVXdZcwu677VnYbca47/ecXPLO7hOGMuMY7j2cj9ruTY8r8Otf4iP9bZV2PKUIpAAAAbdynFAAAgJVyn1IAAADWklDKMB695dbuEoai3/PT83mN2u8Rxz3imCPGHPeIY44w7pGs65iFUmZ13113dpcwnBF7PuKYIyIeOPKx7hJmN+pcjzjuEcccYdwjGXHMEWOOe8QxR0Rc8JXPvuxU+4RShrH/Da/vLmEo+j2/5/yt67pLGMqoa3zEcY845ogxxz3imCOMeyTrOmYXOgIAAGClXOgIAACAtSSUAgAA0EYoBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG2EUgAAANoIpQAAALQRSmHDHT78tu4SGIB1Ng5zPQ5zPQ5zPY51nWuhFAAAgDZZVd01xGKxqOVy2V0GAAAAK5CZO1W12G2fM6UAAAC0EUoBAABoI5QCAADQRigF2BD333u4uwTYSN5bsDreX0QIpQAAADRy9V0AAABWytV3AQAAWEtCKQAAAG2EUoZx88GPdpcwFP2en57Pa9R+jzjuEcccMea4RxxzhHGPZF3HLJQCAADQxoWOAAAAWCkXOoJm7sE1DnM9L/2en57PS7/npd/z0/N5rWu/hVIAAADa+PguAAAAK+XjuwAAAKwloRQAAIA2QikAAABthFIAAADaCKUwg9seuK27BGZiruel3/PT83np97z0e356Pq917bdQCgAAQBu3hAEAAGCl3BIGmh07eKS7BGZiruel3/PT83np97z0e356Pq917bdQCrCHfudP399dAqyUNc4ms77ZdOu6xoVSmMEF11/VXQIzedY1F3SXMBTvrflZ4/Oyxudlfc/PGp/Xuq5x3ykFAABgpXynFBjKzQc/2l0CrJQ1zqazxpmDdbY+hFIAAADaCKXAxvnh67+2u4ThPHrLrd0lDMUaZ9P94z96T3cJDMCxdH0IpQAAALR5ylCamc/OzPsz8w8y8w8z819P26/OzPdl5qHM/JXMfOa0/VnT80PT/gMrHgMAzfa/4fXdJQAbxDEFxnI6Z0r/MiK+s6q+JSJeGBEvy8wXR8RPR8TNVfWCiPhcRNw0vf6miPjctP3m6XUAAABwkqcMpXXcF6anz5j+VER8Z0T86rT9joh45fT4hul5TPtfmpm5VwUDAACwOU7rO6WZeV5mPhARj0TEwYj4k4j4fFU9Pr3koYi4fHp8eUQ8GBEx7T8WEV+9hzXDGXnrx492lzAU/Z6fns9r1H6POO4Rxxwx5rhHHHOEcY9kXcd8WqG0qv66ql4YEVdExHUR8fWrLArgbLjvGHO4f/nJ7hIYgOMZc3A8Yw7nPffiy06172ldfbeqPh8R2xHxtyPiwszcN+26IiIenh4/HBFXRkRM+y+IiM88vZJh7/3o1Zd2lzAU/Z6fns/r2y58bncJLUZcZyOOOWLMcY845gjHs5Gs65izqp78BZn7I+KvqurzmfmVEfGeOH7xohsj4teq6pcz8+cj4gNVdVtmvi4ivqmq/kVmvjoi/mFVverJfsdisajlcrknAwIAAGC9ZOZOVS1227dvt41f5tKIuCMzz4vjZ1bfVVW/mZkfjohfzsx/GxHvj4jbp9ffHhH/LTMPRcRnI+LVZz0CAAAANtLpXH33A1X1rVX1zVX1jVX1b6bth6vquqp6QVV9b1X95bT9/0zPXzDtP7zqQQCc6P57HXbYbNY4m84aZw7W2fp4Wt8pBQAAgL30lN8pnYPvlAIAAGyuJ/tOqTOlAAAAtBFKAQAAaCOUwhy239JdAXMx1/PS7/np+bz0e176PT89n9ea9lsoBQAAoI0LHQEAALBSLnQEAADAWhJKAQAAaCOUAgAA0EYoBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG2EUgAAANoIpQAAALQRSmHD3XfXnd0lMBNzPQ5zPS/9np+ez0u/56XfJxNKAQAAaJNV1V1DLBaLWi6X3WUAwDlhe3s7tra2ussAOKc5ls4rM3eqarHbPmdKgY1z+PDbukuAlfpi3d1dAgNwLGUO1hkRQikAnHOuPnB1dwkA5zxnSdeHj+8CAACwUj6+CwAAwFoSSoGVePSWW7tLYADWGXOwzsZhrsdivteHUAoAAEAb3ykFAABgpXynFJodO3iku4Sh6Pf89Hxe+j0v/Z6fns9Lv+el3ycTSgEAAGjj47sAAACslI/vAgAAsJaEUgAAANoIpQAAALQRSgEAAGgjlAIAANBGKIUZuB/VvPR7fno+L/2el37PT8/npd/z0u+TCaXASmxvb3eXwACss3GY67H8zp++v7sEZuK9PY7zzz//slPtE0phBhdcf1V3CUPR7/np+bz0e176Pb9nXXNBdwlDscbnpd8ny6rqriEWi0Utl8vuMgAAAFiBzNypqsVu+5wpBQAAoI1QChvutgdu6y6BmZjrcZjrcZjrcZjrcZjrkwmlAAAAtBFKYQb333u47Xf/4At/sO13j8hcz6uz350653rUnndZPPiy7hKG4hg+vxGPKY7hJxNKAQAAaOPquwDA2jt28IjbKACcw1x9FwAAgLUklAIAa89ZUoDNJZQCAADQRigFAACgjVAKwDnLDciBveSYAj2EUgAAANq4JQwAAAAr5ZYwAAAArCWhFAAAgDZCKQAAAG2EUgAAANoIpQAA0G37Ld0VQBuhFAAAgDZCKQAAdNt6c3cF0EYoBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG2EUgAAANoIpQAAALQRSgEAAGgjlAIAANBGKAUAABjA/fce7i5hV0IpAAAnue+uO7tLAAYhlAIAAAzguu+5pruEXQmlAACc5Nu+9/u6SwAGIZQCAADQRigFAACgjVAKAABAG6EUAACANkIpAAAAbU47lGbmeZn5/sz8zen51Zn5vsw8lJm/kpnPnLY/a3p+aNp/YEW1A2tsXW/OzGaxzpiDdTYOcz0W870+ns6Z0jdGxEdOeP7TEXFzVb0gIj4XETdN22+KiM9N22+eXgcAAAAnOa1QmplXRMTfjYj/Oj3PiPjOiPjV6SV3RMQrp8c3TM9j2v/S6fXAQDpvznzs4JG23z2q7e3tlt+7rjcBXzVrfF6jrrOu93VE3xofda5H1TXfncfwzvf1kzndM6U/GxH/KiK+OD3/6oj4fFU9Pj1/KCIunx5fHhEPRkRM+49NrwcAAIAv8ZShNDP/XkQ8UlU7M9QDcNYuuP6q7hKGs7W11V3CUKxx5tD5vrbG2WSd63td/73edxqv+faI+PuZ+YqIeHZEnB8Rb4uICzNz33Q29IqIeHh6/cMRcWVEPJSZ+yLigoj4zJ5XDgAAwDnvKc+UVtWbq+qKqjoQEa+OiN+qqu+LiO2I+EfTy26MiLunx/dMz2Pa/1tVVXtaNZyJ7bd0VzCWUftt3OMYccwRY457xDFHjDnuEcccYdwjWdMxn819Sn8sIn4kMw/F8e+M3j5tvz0ivnra/iMR8aazKxEAAIBNletwEnOxWNRyuewuAwBYU4/ecmvsf8Pru8sA4Axd+oxnHD36V3912W77zuZMKQAAAJwVoRQAWHvOkgKc2z71+OOfPNU+oRQAAIA2QimwEocPv627BGZkvsdhrsdhrsdhrsexrnMtlAIAANBGKAVW4ppr3thdAjMy3+O4+09e3l0CM/G+Hoe5Hse6HsOFUgAAANq4TykAAAArlZk7VbXYbZ8zpQAAALQRSgEAAGgjlAIAANBGKAUAAKCNUAoAAEAboRQAAIA2QikAAABthFIAAADaCKUAAAC0EUoBOGfdd9ed3SUAG8QxBXoIpQAAALTJququIRaLRS2Xy+4yAAAAWIHM3KmqxW77nCmFDXf48Nu6S2Am5noc5noc5noc5noc5vpkQikAAABtfHwXAACAlfLxXQAAANaSUAoAAEAboRQAAIA2QikAAABthFIAAADaCKUAAAC0EUoBAABoI5QCAADQRigFAACgjVAKAABAG6EUAACANkIpAAAAbYRSAGDtHTt4pLsEAFZEKAUAAKCNUAoArL0Lrr+quwQAVkQoBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG2EUgAAANoIpQAAALQRSgEAAGgjlAIAANBGKAUAAKCNUAoAAEAboZRxbL+lu4Kx6Pf89Hxeo/Z7xHGPOOaIMcc94pgjjHskazpmoRQAAIA2WVXdNcRisajlctldBgAAACuQmTtVtdhtnzOlAAAAtBFKAQAAaCOUAuyh7e3t7hJgI3lvsemscTbd+eeff9mp9gmlAAAAtBFKGcZbP360u4ShjNrv5YGv7y6hxYjzPeKYI/rGvbW11fJ7I8z1SDrHbI3Pb8Rxd475scce++Sp9gmlAAAAtHFLGAAAAFbKLWEAAABYS0IpAAAAbYRSAAAA2gilDOPYwSPdJQxl1H4b9zhGHHPEmOMeccwRY457xDFHGPdI1nXMQikAAABtXH0XAACAlXL1XQAAANaSUAoAAEAboRQAAIA2QinM4P57D3eXwEzM9bz0e356Pi/9npd+z0/P57Wu/RZKgZW47647u0tgAEf+4N3dJTAAxzPmYJ0xh3X9d1MohRlc9z3XdJfATMz1vC7/uou6SxiONT4v/Z6Xfs9Pz+e1rv9uuiUMAAAAK+WWMAAAAKwloRQAAIA2QikAAABthFIAAADaCKUAAAC0Oa1QmpmfyMwPZuYDmbmctj0vMw9m5semnxdN2zMzfy4zD2XmBzLzRascAAAAAOeup3OmdKuqXnjCZXzfFBHvraprI+K90/OIiJdHxLXTn9dGxNv3qlgAAAA2y9l8fPeGiLhjenxHRLzyhO3vrON+NyIuzMxLz+L3AGfh2MEj3SUwE3M9DnM9DnM9DnM9DnN9stMNpRUR78nMncx87bTtkqo6Oj3+VERcMj2+PCIePOHvPjRtg1aHD7+tu4Sh6Pf8Hs5f6C5hKNb4vPR7fno+L8fw+Vnj89q//7zLTrVv32n+N76jqh7OzL8REQcz849O3FlVlZl1NkUCq3HB9Vd1l8BMnn3Nhd0lMBPv63GY63E4ho/D+/pkWfX0smRm/mREfCEi/llEvKSqjk4fz/2fVfV1mfmfpse/NL3+j5943an+m4vFopbL5ZmOAQAAgDWWmTsnXJ/oSzzlx3cz8zmZ+dwnHkfEd0fEhyLinoi4cXrZjRFx9/T4noj4/ukqvC+OiGNPFkgBAAAY1+l8fPeSiPiNzHzi9b9YVf8jM38vIt6VmTdFxJGIeNX0+ndHxCsi4lBE/EVE/MCeVw0AAMBGeMpQWlWHI+Jbdtn+mYh46S7bKyJetyfVAQAAsNHO5pYwAAAAcFaEUgAAANoIpcDG2d7e7i6BAVhnzME6Yw7W2TjWda6FUoA95EbcbDprnE1mfbPpvlh3P/WLGgilDOPmgx/tLmEonf3e2tpq+92drPF5PfD45d0lDGXU9T3qOhtxvkf9d9Ncz+vqA1e3/e7znnvxZafaJ5QC7KFrrnljdwmwUtY4m8z6ZtOt6xrP43dwaS4i89E4fq9TAAAANs9VVbV/tx1rEUoBAAAYk4/vAgAA0EYoBQAAoI1QCgAAQBuhFAAAgDZCKQAAAG3aQ2lmviwz/zgzD2Xmm7rrYTNl5icy84OZ+UBmLrvrYTNk5jsy85HM/NAJ256XmQcz82PTz4s6a+Tcd4p19pOZ+fB0THsgM1/RWSPntsy8MjO3M/PDmfmHmfnGabvjGXvmSdaZ4xm9t4TJzPMi4qMRcX1EPBQRvxcRr6mqD7cVxUbKzE9ExKKq/qy7FjZHZv6diPhCRLyzqr5x2vbvI+KzVfVT0//RdlFV/VhnnZzbTrHOfjIivlBV/6GzNjZDZl4aEZdW1e9n5nMjYiciXhkR/zQcz9gjT7LOXhWOZ8PrPlN6XUQcqqrDVfV/I+KXI+KG5poATktV/XZEfPbLNt8QEXdMj++I4//gwhk7xTqDPVNVR6vq96fHfx4RH4mIy8PxjD30JOsM2kPp5RHx4AnPHwqLk9WoiHhPZu5k5mu7i2GjXVJVR6fHn4qISzqLYaO9PjM/MH2818cq2ROZeSAivjUi3heOZ6zIl62zCMez4XWHUpjLd1TViyLi5RHxuunjcLBSdfz7EX3fkWCTvT0iviYiXhgRRyPiP7ZWw0bIzK+KiF+LiB+qqsdO3Od4xl7ZZZ05ntEeSh+OiCtPeH7FtA32VFU9PP18JCJ+I45/dBxW4dPT92ae+P7MI831sIGq6tNV9ddV9cWI+C/hmMZZysxnxPGgcGdV/fq02fGMPbXbOnM8I6I/lP5eRFybmVdn5jMj4tURcU9zTWyYzHzO9IX6yMznRMR3R8SHnvxvwRm7JyJunB7fGBF3N9bChnoiKEz+QTimcRYyMyPi9oj4SFX9zAm7HM/YM6daZ45nRDRffTciYrrs889GxHkR8Y6q+netBbFxMvOaOH52NCJiX0T8onXGXsjMX4qIl0TExRHx6Yj4iYj47xHxroj4mxFxJCJeVVUuUsMZO8U6e0kc/6hbRcQnIuKfn/DdP3haMvM7IuJ/RcQHI+KL0+Yfj+Pf93M8Y088yTp7TTieDa89lAIAADCu7o/vAgAAMDChFAAAgDZCKQAAAG2EUgAAANoIpQAAALQRSgEAAGgjlAIAANBGKAUAAKDN/wMrxyrVgXBv9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(16,8))\n",
    "rasterplot(np.arange(0, n_steps), data[layer_probes[0]][0, :, np.random.choice(5408, 512, replace=False)].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-sweden",
   "metadata": {},
   "source": [
    "# Print the spikes in `Conv0` layer (only if their amplitude is more than 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "earlier-origin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv0_spikes_matrix = data[layer_probes[0]] * sfr * 0.001\n",
    "test_image_index = 0\n",
    "\n",
    "for neuron in range(data[layer_probes[0]].shape[2]):\n",
    "  if np.any(conv0_spikes_matrix[test_image_index, :, neuron]):\n",
    "    spikes=np.round(conv0_spikes_matrix[test_image_index, :, neuron])\n",
    "    if set(np.unique(spikes)) - set([0, 1]):\n",
    "      print(spikes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
