{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reserved-uzbekistan",
   "metadata": {},
   "source": [
    "### Training and testing with `scale_firing_rates`, and `LoihiSpikingRectifiedLinear()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abstract-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "committed-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "  tf.keras.datasets.mnist.load_data()\n",
    ")\n",
    "# Make it channels_first.\n",
    "train_images, test_images = np.expand_dims(train_images, -1), np.expand_dims(test_images, -1)\n",
    "train_images, test_images = np.moveaxis(train_images, -1, 1), np.moveaxis(test_images, -1, 1)\n",
    "\n",
    "# Flatten images and add time dimension.\n",
    "train_images = train_images.reshape((train_images.shape[0], 1, -1))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1, -1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 1, -1))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], 1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-timer",
   "metadata": {},
   "source": [
    "# Create the TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abroad-tablet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1, 28, 28)]       0         \n",
      "_________________________________________________________________\n",
      "to-spikes (Conv2D)           (None, 3, 28, 28)         3         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 8, 26, 26)         216       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 16, 12, 12)        1152      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense0 (Dense)               (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 149,541\n",
      "Trainable params: 149,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input(shape=(1, 28, 28), name=\"input\")\n",
    "\n",
    "to_spikes = tf.keras.layers.Conv2D(\n",
    "  filters=3, # 3 RGB Neurons per pixel.\n",
    "  kernel_size=(1, 1), strides=(1, 1), activation=tf.nn.relu, use_bias=False, # Default is True.\n",
    "  data_format=\"channels_first\", name=\"to-spikes\")(inp)\n",
    "\n",
    "conv0 = tf.keras.layers.Conv2D(\n",
    "  filters=8, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv0\")(to_spikes)\n",
    "\n",
    "# maxp0 = tf.keras.layers.MaxPool2D(\n",
    "#   pool_size=(2, 2), padding=\"valid\", data_format=\"channels_first\", name=\"MaxPool0\")(conv0)\n",
    "\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "  filters=16, kernel_size=(3, 3), strides=(2, 2), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv1\")(conv0)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten(name=\"flatten\")(conv1)\n",
    "dense0 = tf.keras.layers.Dense(64, activation=tf.nn.relu, name=\"dense0\")(flatten)\n",
    "\n",
    "#dense1 = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"dense1\")(dense0) # Results in formation of TensorNode\n",
    "                                                                                # which isn't supported on Loihi.\n",
    "dense1 = tf.keras.layers.Dense(10, name=\"dense1\")(dense0)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=dense1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-practice",
   "metadata": {},
   "source": [
    "# Convert TF Model to Nengo DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conscious-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nengo_dl_model(is_test=False, **kwargs):\n",
    "  converter = nengo_dl.Converter(model, **kwargs)\n",
    "#   if is_test:\n",
    "#     #converter.net._connections[2].synapse=nengo.Lowpass(0.005)\n",
    "#     for conn in converter.net._connections:\n",
    "#       print(conn, conn.synapse)\n",
    "  return converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aboriginal-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params_file=\"./attempting_TN_MP_loihineurons_8_16\", epochs=1, **kwargs):\n",
    "  converter = _get_nengo_dl_model(**kwargs)\n",
    "  \n",
    "  with nengo_dl.Simulator(converter.net, seed=0, minibatch_size=200) as sim:\n",
    "    sim.compile(\n",
    "      optimizer=tf.optimizers.Adam(0.001),\n",
    "      loss={\n",
    "        converter.outputs[dense1]: tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "      },\n",
    "      metrics={converter.outputs[dense1]: tf.metrics.sparse_categorical_accuracy},\n",
    "    )\n",
    "    sim.fit(\n",
    "      {converter.inputs[inp]: train_images},\n",
    "      {converter.outputs[dense1]: train_labels},\n",
    "      epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # save the parameters to file.\n",
    "    sim.save_params(params_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neutral-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(activation, params_file=\"./attempting_TN_MP_loihineurons_8_16\", n_steps=30,\n",
    "                 scale_firing_rates=1, synapse=None, n_test=100):\n",
    "  nengo_converter = _get_nengo_dl_model(\n",
    "    is_test=True,\n",
    "    scale_firing_rates=scale_firing_rates, \n",
    "    swap_activations={tf.nn.relu: activation},\n",
    "    synapse=synapse\n",
    "  )\n",
    "  \n",
    "  tiled_test_images = np.tile(test_images[:n_test], (1, n_steps, 1))\n",
    "  # Speed up simulation.\n",
    "  with nengo_converter.net:\n",
    "    nengo_dl.configure_settings(stateful=False)\n",
    "    \n",
    "  # Build network, load in trained weights, do inference.\n",
    "  with nengo_dl.Simulator(\n",
    "      nengo_converter.net, minibatch_size=20, progress_bar=False) as sim:\n",
    "    sim.load_params(params_file)\n",
    "    data = sim.predict({nengo_converter.inputs[inp]: tiled_test_images})\n",
    "    \n",
    "  test_predictions = np.argmax(data[nengo_converter.outputs[dense1]][:, -1], axis=-1)\n",
    "  print(\"Nengo DL Test Acc: %s\" % (100 * np.mean(test_predictions == test_labels[:n_test, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-tiffany",
   "metadata": {},
   "source": [
    "# Train Nengo-DL Model with `LoihiSpikingRectifiedLinear()` neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fundamental-collins",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/4\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.2512 - probe_loss: 0.2512 - probe_sparse_categorical_accuracy: 0.9247\n",
      "Epoch 2/4\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.0851 - probe_loss: 0.0851 - probe_sparse_categorical_accuracy: 0.9740\n",
      "Epoch 3/4\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.0607 - probe_loss: 0.0607 - probe_sparse_categorical_accuracy: 0.9811\n",
      "Epoch 4/4\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.0487 - probe_loss: 0.0487 - probe_sparse_categorical_accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "  params_file=\"./attempting_TN_MP_loihineurons_8_16\",\n",
    "  epochs=4,\n",
    "  swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()},\n",
    "  scale_firing_rates=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-rwanda",
   "metadata": {},
   "source": [
    "# Test Nengo-DL Model with `LoihiSpikingRectifiedLinear()` neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "radical-compromise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nengo DL Test Acc: 100.0\n"
     ]
    }
   ],
   "source": [
    "test_network(\n",
    "  activation=nengo_loihi.neurons.LoihiSpikingRectifiedLinear(),\n",
    "  scale_firing_rates=100,\n",
    "  synapse=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-forty",
   "metadata": {},
   "source": [
    "# Convert TF Model to NengoLoihi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vulnerable-grass",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n"
     ]
    }
   ],
   "source": [
    "pres_time = 0.03\n",
    "n_test = 5\n",
    "\n",
    "nengo_converter_2 = _get_nengo_dl_model(\n",
    "    is_test=True,\n",
    "    scale_firing_rates=100, \n",
    "    swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()},\n",
    "    synapse=0.005\n",
    "  )\n",
    "nengo_input_2 = nengo_converter_2.inputs[inp]\n",
    "nengo_output_2 = nengo_converter_2.outputs[dense1]\n",
    "\n",
    "net_2 = nengo_converter_2.net\n",
    "with nengo_dl.Simulator(net_2) as sim_2:\n",
    "  sim_2.load_params(\"./attempting_TN_MP_loihineurons_8_16\")\n",
    "  sim_2.freeze_params(net_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "forty-seven",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b44c8544610>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b44c856ea50>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b44cb9f7b10>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b44c847d190>, <tensorflow.python.keras.layers.core.Flatten object at 0x2b44cbc87990>, <tensorflow.python.keras.layers.core.Dense object at 0x2b44cbc893d0>, <tensorflow.python.keras.layers.core.Dense object at 0x2b44c8476510>]\n"
     ]
    }
   ],
   "source": [
    "converter_layers = nengo_converter_2.model.layers\n",
    "print(converter_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "progressive-circuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 26, 26), (16, 12, 12), (64,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv0_shape = converter_layers[2].output.shape[1:]\n",
    "conv1_shape = converter_layers[3].output.shape[1:] # 4 with MP layers.\n",
    "dense0_shape = converter_layers[5].output.shape[1:] # 6 with MP layers.\n",
    "\n",
    "tuple(conv0_shape), tuple(conv1_shape), tuple(dense0_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-greece",
   "metadata": {},
   "source": [
    "# Configure the NengoLoihi settings/block shapes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "contained-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_layers = nengo_converter_2.model.layers\n",
    "with net_2:\n",
    "  nengo_input_2.output = nengo.processes.PresentInput(\n",
    "    test_images, presentation_time=pres_time\n",
    "  )\n",
    "  nengo_loihi.add_params(net_2)\n",
    "  \n",
    "  # Set the to_spikes layer Off-Chip.\n",
    "  net_2.config[nengo_converter_2.layers[to_spikes].ensemble].on_chip = False\n",
    "  # Set the maxp0 layer Off-Chip.\n",
    "  # net_2.config[nengo_converter_2.layers[maxp0].ensemble].on_chip = False\n",
    "  \n",
    "  conv0_shape = tuple(converter_layers[2].output.shape[1:])\n",
    "  conv1_shape = tuple(converter_layers[3].output.shape[1:])\n",
    "  dense0_shape = tuple(converter_layers[5].output.shape[1:])\n",
    "  \n",
    "  net_2.config[\n",
    "    nengo_converter_2.layers[conv0].ensemble\n",
    "  ].block_shape = nengo_loihi.BlockShape((4, 16, 16), conv0_shape)\n",
    "  net_2.config[nengo_converter_2.layers[conv1].ensemble].block_shape = (\n",
    "      nengo_loihi.BlockShape((8, 11, 11), conv1_shape))\n",
    "  net_2.config[nengo_converter_2.layers[dense0].ensemble].block_shape = (\n",
    "      nengo_loihi.BlockShape((32,), dense0_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-morgan",
   "metadata": {},
   "source": [
    "# Build the NengoLoihi Network and Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "subtle-eligibility",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nengo Loihi Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "with nengo_loihi.Simulator(net_2, target=\"sim\") as loihi_sim:\n",
    "  loihi_sim.run(n_test * pres_time)\n",
    "  \n",
    "  # get output (last timestep of each presentation period)\n",
    "  pres_steps = int(round(pres_time / loihi_sim.dt))\n",
    "  output = loihi_sim.data[nengo_output_2][pres_steps - 1 :: pres_steps]\n",
    "  \n",
    "  # compute Loihi accuracy\n",
    "  loihi_predictions = np.argmax(output, axis=-1)\n",
    "  correct = 100 * np.mean(loihi_predictions == test_labels[:n_test, 0, 0])\n",
    "  print(\"Nengo Loihi Accuracy: %s\" % correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
