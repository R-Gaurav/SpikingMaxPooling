{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "environmental-sacrifice",
   "metadata": {},
   "source": [
    "### With `scale_firing_rates` during training, and with `LoihiSpikingRectifiedLinear()` but  no regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-rebel",
   "metadata": {},
   "source": [
    "Do not set `amplitude` and `max_rates` as you are not creating `Ensembles` from scratch, rather, converting a trained TF Model.\n",
    "\n",
    "`Dropout` and `padding=\"same\"` not supported in NengoLoihi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "theoretical-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nengo.utils.matplotlib import rasterplot\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-conference",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "  tf.keras.datasets.mnist.load_data()\n",
    ")\n",
    "# Make it channels_first.\n",
    "train_images, test_images = np.expand_dims(train_images, -1), np.expand_dims(test_images, -1)\n",
    "train_images, test_images = np.moveaxis(train_images, -1, 1), np.moveaxis(test_images, -1, 1)\n",
    "\n",
    "# Flatten images and add time dimension.\n",
    "train_images = train_images.reshape((train_images.shape[0], 1, -1))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1, -1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 1, -1))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], 1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-parent",
   "metadata": {},
   "source": [
    "# Design the TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "found-artwork",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1, 28, 28)]       0         \n",
      "_________________________________________________________________\n",
      "to-spikes (Conv2D)           (None, 3, 28, 28)         3         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 8, 26, 26)         216       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 16, 12, 12)        1152      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense0 (Dense)               (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 149,541\n",
      "Trainable params: 149,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input(shape=(1, 28, 28), name=\"input\")\n",
    "\n",
    "to_spikes = tf.keras.layers.Conv2D(\n",
    "  filters=3, # 3 RGB Neurons per pixel.\n",
    "  kernel_size=(1, 1), strides=(1, 1), activation=tf.nn.relu, use_bias=False, # Default is True.\n",
    "  data_format=\"channels_first\", name=\"to-spikes\")(inp)\n",
    "\n",
    "conv0 = tf.keras.layers.Conv2D(\n",
    "  filters=8, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv0\")(to_spikes) # padding=\"same\" not allowed in NengoLoihi, default \"valid\"\n",
    "# Dropout here leads to: \"BuildError: Conv2D transforms not supported for off-chip to on-chip connections where \n",
    "# `pre` is not a Neurons object.\" in NengoLoihi.\n",
    "\n",
    "# dp_out = tf.keras.layers.Dropout(0.2)(conv0) \n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "  filters=16, kernel_size=(3, 3), strides=(2, 2), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv1\")(conv0)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten(name=\"flatten\")(conv1)\n",
    "# Dropout here leads to: \"BuildError: Input axons (4608) exceeded max (4096) in \n",
    "# LoihiBlock(<Ensemble \"dense0.0\">[0:22:1])\" in NengoLoihi.\n",
    "#dp_out = tf.keras.layers.Dropout(0.2)(flatten) \n",
    "dense0 = tf.keras.layers.Dense(64, activation=tf.nn.relu, name=\"dense0\")(flatten)\n",
    "\n",
    "#dense1 = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"dense1\")(dense0) # Results in formation of TensorNode\n",
    "                                                                                # which isn't supported on Loihi.\n",
    "# Dropout here leads to: \"SimulationError: Cannot call TensorNode output function (this probably means you are trying\"\n",
    "# to use a TensorNode inside a Simulator other than NengoDL)\" in NengoLoihi.\n",
    "dp_out = tf.keras.layers.Dropout(0.2)(dense0)\n",
    "dense1 = tf.keras.layers.Dense(10, name=\"dense1\")(dp_out)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=dense1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thirty-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nengo_dl_model(set_probes=True, **kwargs):\n",
    "  converter = nengo_dl.Converter(model, **kwargs)\n",
    "  lyr_probes = []\n",
    "  # With a SpikingRectifiedLinear() neuron in off-chip layers, the later layer neurons start spiking earlier.\n",
    "  # In some cases, it degrades the performance.\n",
    "  #with converter.net:\n",
    "  #  converter.layers[to_spikes].ensemble.neuron_type = nengo_loihi.neurons.SpikingRectifiedLinear(\n",
    "  #   amplitude=1) # If this neuron is used, then amplitude is required to be 1/scale_firing_rates.\n",
    "    \n",
    "  if set_probes:\n",
    "    with converter.net:\n",
    "      probe_conv0 = nengo.Probe(converter.layers[conv0])\n",
    "      probe_conv1 = nengo.Probe(converter.layers[conv1])\n",
    "      probe_dense0 = nengo.Probe(converter.layers[dense0])\n",
    "      lyr_probes.extend([probe_conv0, probe_conv1, probe_dense0])\n",
    "  return converter, lyr_probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-hello",
   "metadata": {},
   "source": [
    "# Train the model and save the Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "choice-frame",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "|#                        Constructing graph                          | 0:00:00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Layer type <class 'tensorflow.python.keras.layers.core.Dropout'> does not have a registered converter. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/4\n",
      "300/300 [==============================] - 3s 9ms/step - loss: 0.3009 - probe_loss: 0.3009 - probe_accuracy: 0.9087\n",
      "Epoch 2/4\n",
      "300/300 [==============================] - 3s 9ms/step - loss: 0.1036 - probe_loss: 0.1036 - probe_accuracy: 0.9688\n",
      "Epoch 3/4\n",
      "300/300 [==============================] - 3s 9ms/step - loss: 0.0780 - probe_loss: 0.0780 - probe_accuracy: 0.9761\n",
      "Epoch 4/4\n",
      "300/300 [==============================] - 3s 9ms/step - loss: 0.0616 - probe_loss: 0.0616 - probe_accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "params_file, epochs = \"./attempting_TN_MP_loihineurons_8_16\", 4\n",
    "\n",
    "converter, lyr_probes = _get_nengo_dl_model(\n",
    "  #swap_activations={tf.nn.relu: nengo_loihi.neurons.SpikingRectifiedLinear()}, # With this neuron, spike amplitude\n",
    "                                                                                # is more than 1.\n",
    "  swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}, # Spike amplitude is just 1.\n",
    "  scale_firing_rates=100 # Required, else NengoLoihi performance is very poor.\n",
    ")\n",
    "  \n",
    "with nengo_dl.Simulator(converter.net, seed=0, minibatch_size=200) as sim:\n",
    "    losses = {\n",
    "      converter.outputs[dense1]: tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      converter.outputs[dense1]: \"accuracy\"\n",
    "    }\n",
    "    \n",
    "    sim.compile(\n",
    "      loss=losses,\n",
    "      optimizer=tf.optimizers.Adam(0.001),\n",
    "      #metrics={converter.outputs[dense1]: tf.metrics.sparse_categorical_accuracy},\n",
    "      metrics=metrics,\n",
    "    )\n",
    "    sim.fit(\n",
    "      {converter.inputs[inp]: train_images},\n",
    "      {converter.outputs[dense1]: train_labels},\n",
    "      epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # save the parameters to file.\n",
    "    sim.save_params(params_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-german",
   "metadata": {},
   "source": [
    "# Print the `gain`, `bias`, `intercepts`, and `max_rates` of the `Dense0` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acting-powell",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/simulator.py:2266: UserWarning: Checking parameters after simulator is closed; cannot fetch live values, so the initial values will be returned.\n",
      "  \"Checking parameters after simulator is closed; \"\n"
     ]
    }
   ],
   "source": [
    "print(sim.data[converter.layers[dense0].ensemble].gain)\n",
    "print(sim.data[converter.layers[dense0].ensemble].bias)\n",
    "print(sim.data[converter.layers[dense0].ensemble].intercepts)\n",
    "print(sim.data[converter.layers[dense0].ensemble].max_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-dream",
   "metadata": {},
   "source": [
    "# Test the NengoDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nengo DL Test Acc: 87.0\n"
     ]
    }
   ],
   "source": [
    "n_steps=30\n",
    "synapse=0.005\n",
    "n_test=100\n",
    "sfr=100\n",
    "\n",
    "nengo_converter, layer_probes = _get_nengo_dl_model(\n",
    "    scale_firing_rates=sfr, \n",
    "    #swap_activations={tf.nn.relu: nengo_loihi.neurons.SpikingRectifiedLinear()}, # With this neuron, spike \n",
    "                                                                                  # amplitude is more than 1.\n",
    "    swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}, # Spike Amplitude is just 1.\n",
    "    synapse=synapse\n",
    ")\n",
    "  \n",
    "tiled_test_images = np.tile(test_images[:n_test], (1, n_steps, 1))\n",
    "# Speed up simulation.\n",
    "with nengo_converter.net:\n",
    "  nengo_dl.configure_settings(stateful=False)\n",
    "    \n",
    "# Build network, load in trained weights, do inference.\n",
    "with nengo_dl.Simulator(\n",
    "    nengo_converter.net, minibatch_size=20, progress_bar=False) as sim:\n",
    "  sim.load_params(params_file)\n",
    "  data = sim.predict({nengo_converter.inputs[inp]: tiled_test_images})\n",
    "    \n",
    "test_predictions = np.argmax(data[nengo_converter.outputs[dense1]][:, -1], axis=-1)\n",
    "print(\"Nengo DL Test Acc: %s\" % (100 * np.mean(test_predictions == test_labels[:n_test, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-engine",
   "metadata": {},
   "source": [
    "# Plotting the spikes in Conv0 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decent-latitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(16,8))\n",
    "rasterplot(np.arange(0, n_steps), data[layer_probes[0]][0, :, np.random.choice(5408, 512, replace=False)].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-virus",
   "metadata": {},
   "source": [
    "# Print the spikes in `Conv0` layer (only if their amplitude is more than 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fabulous-hughes",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv0_spikes_matrix = data[layer_probes[0]] * sfr * 0.001\n",
    "test_image_index = 0\n",
    "\n",
    "for neuron in range(data[layer_probes[0]].shape[2]):\n",
    "  if np.any(conv0_spikes_matrix[test_image_index, :, neuron]):\n",
    "    spikes=np.round(conv0_spikes_matrix[test_image_index, :, neuron])\n",
    "    if set(np.unique(spikes)) - set([0, 1]):\n",
    "      print(spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-visitor",
   "metadata": {},
   "source": [
    "# Inferencing with NengoLoihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "outdoor-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Layer type <class 'tensorflow.python.keras.layers.core.Dropout'> does not have a registered converter. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n"
     ]
    }
   ],
   "source": [
    "nengo_converter1, layer_probes1 = _get_nengo_dl_model(\n",
    "    scale_firing_rates=sfr, \n",
    "    #swap_activations={tf.nn.relu: nengo_loihi.neurons.SpikingRectifiedLinear()}, # With this neuron, spike \n",
    "                                                                                  # amplitude is more than 1.\n",
    "    swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}, # Spike Amplitude is just 1.\n",
    "    synapse=synapse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "better-beast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n"
     ]
    }
   ],
   "source": [
    "pres_time = n_steps * 0.001\n",
    "net1 = nengo_converter1.net\n",
    "ndl_input1 = nengo_converter1.inputs[inp]\n",
    "ndl_output1 = nengo_converter1.outputs[dense1]\n",
    "\n",
    "with nengo_dl.Simulator(net1) as ndl_sim1:\n",
    "  ndl_sim1.load_params(params_file)\n",
    "  ndl_sim1.freeze_params(net1)\n",
    "\n",
    "with net1:\n",
    "  ndl_input1.output = nengo.processes.PresentInput(\n",
    "    test_images, presentation_time=pres_time\n",
    "  )\n",
    "  \n",
    "  nengo_loihi.add_params(net1)\n",
    "  net1.config[nengo_converter1.layers[to_spikes].ensemble].on_chip = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "resident-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_loihi/simulator.py:148: UserWarning: Model is precomputable. Setting precompute=False may slow execution.\n",
      "  \"Model is precomputable. Setting precompute=False may slow execution.\"\n"
     ]
    },
    {
     "ename": "SimulationError",
     "evalue": "Cannot call TensorNode output function (this probably means you are trying to use a TensorNode inside a Simulator other than NengoDL)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSimulationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1baeaad6102b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnengo_loihi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mloihi_sim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#precompute=False, hardware_options=hw_opts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mloihi_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_test\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpres_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mpres_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpres_time\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mloihi_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_loihi/simulator.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_in_seconds)\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             )\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_loihi/simulator.py\u001b[0m in \u001b[0;36mrun_steps\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSimulatorClosed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Simulator cannot run because it is closed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished running for %d steps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_loihi/simulator.py\u001b[0m in \u001b[0;36memu_bidirectional_with_host\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host2chip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo/simulator.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                 \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mold_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo/builder/operator.py\u001b[0m in \u001b[0;36mstep_simpyfunc\u001b[0;34m()\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_simpyfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/tensor_node.py\u001b[0m in \u001b[0;36moutput_func\u001b[0;34m(*_)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0moutput_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             raise SimulationError(\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0;34m\"Cannot call TensorNode output function (this probably means \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0;34m\"you are trying to use a TensorNode inside a Simulator other \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;34m\"than NengoDL)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSimulationError\u001b[0m: Cannot call TensorNode output function (this probably means you are trying to use a TensorNode inside a Simulator other than NengoDL)"
     ]
    }
   ],
   "source": [
    "#hw_opts = dict(snip_max_spikes_per_step=300)\n",
    "\n",
    "with nengo_loihi.Simulator(net1, precompute=False) as loihi_sim: #precompute=False, hardware_options=hw_opts\n",
    "  loihi_sim.run(n_test * pres_time)\n",
    "  \n",
    "  pres_steps = int(round(pres_time / loihi_sim.dt))\n",
    "  output = loihi_sim.data[ndl_output1][pres_steps - 1 :: pres_steps]\n",
    "  \n",
    "  # compute the Loihi accuracy\n",
    "  loihi_predictions = np.argmax(output, axis=-1)\n",
    "  correct = 100 * np.mean(loihi_predictions == test_labels[:n_test, 0, 0])\n",
    "  print(\"Loihi accuracy: %.2f%%\" % correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "with net1:\n",
    "  print(nengo_converter1.layers[to_spikes].ensemble.neuron_type)\n",
    "  nengo_converter1.layers[to_spikes].ensemble.neuron_type = nengo_loihi.neurons.SpikingRectifiedLinear()\n",
    "  print(nengo_converter1.layers[to_spikes].ensemble.neuron_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-joining",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
