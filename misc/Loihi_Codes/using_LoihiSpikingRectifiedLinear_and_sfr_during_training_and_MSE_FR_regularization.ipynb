{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "environmental-sacrifice",
   "metadata": {},
   "source": [
    "### With `scale_firing_rates` during training, and with `LoihiSpikingRectifiedLinear()` but  with Mean Squared Error firing rate regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-rebel",
   "metadata": {},
   "source": [
    "Do not set `amplitude` and `max_rates` as you are not creating `Ensembles` from scratch, rather, converting a trained TF Model.\n",
    "\n",
    "Test accuracy is always poor (less than 91%) with `LoihiSpikingRectifiedLinear()` despite all the variations.\n",
    "\n",
    "When you regularize the firing rates, you shouldn't need the `scale_firing_rates` parameter during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "theoretical-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nengo.utils.matplotlib import rasterplot\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-conference",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "  tf.keras.datasets.mnist.load_data()\n",
    ")\n",
    "# Make it channels_first.\n",
    "train_images, test_images = np.expand_dims(train_images, -1), np.expand_dims(test_images, -1)\n",
    "train_images, test_images = np.moveaxis(train_images, -1, 1), np.moveaxis(test_images, -1, 1)\n",
    "\n",
    "# Flatten images and add time dimension.\n",
    "train_images = train_images.reshape((train_images.shape[0], 1, -1))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1, -1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 1, -1))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], 1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-parent",
   "metadata": {},
   "source": [
    "# Design the TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "found-artwork",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1, 28, 28)]       0         \n",
      "_________________________________________________________________\n",
      "to-spikes (Conv2D)           (None, 3, 28, 28)         3         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 8, 26, 26)         216       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 16, 12, 12)        1152      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense0 (Dense)               (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 149,541\n",
      "Trainable params: 149,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input(shape=(1, 28, 28), name=\"input\")\n",
    "\n",
    "to_spikes = tf.keras.layers.Conv2D(\n",
    "  filters=3, # 3 RGB Neurons per pixel.\n",
    "  kernel_size=(1, 1), strides=(1, 1), activation=tf.nn.relu, use_bias=False, # Default is True.\n",
    "  data_format=\"channels_first\", name=\"to-spikes\")(inp)\n",
    "\n",
    "conv0 = tf.keras.layers.Conv2D(\n",
    "  filters=8, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv0\")(to_spikes)\n",
    "\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "  filters=16, kernel_size=(3, 3), strides=(2, 2), activation=tf.nn.relu, use_bias=False,\n",
    "  data_format=\"channels_first\", name=\"conv1\")(conv0)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten(name=\"flatten\")(conv1)\n",
    "dense0 = tf.keras.layers.Dense(64, activation=tf.nn.relu, name=\"dense0\")(flatten)\n",
    "\n",
    "#dense1 = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"dense1\")(dense0) # Results in formation of TensorNode\n",
    "                                                                                # which isn't supported on Loihi.\n",
    "dense1 = tf.keras.layers.Dense(10, name=\"dense1\")(dense0)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=dense1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thirty-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nengo_dl_model(**kwargs):\n",
    "  converter = nengo_dl.Converter(model, **kwargs)\n",
    "  lyr_probes = []\n",
    "  with converter.net:\n",
    "    probe_conv0 = nengo.Probe(converter.layers[conv0])\n",
    "    probe_conv1 = nengo.Probe(converter.layers[conv1])\n",
    "    probe_dense0 = nengo.Probe(converter.layers[dense0])\n",
    "    lyr_probes.extend([probe_conv0, probe_conv1, probe_dense0])\n",
    "  return converter, lyr_probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-hello",
   "metadata": {},
   "source": [
    "# Train the model and save the Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "choice-frame",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 151.2926 - probe_loss: 7.5963 - probe_1_loss: 59770.6328 - probe_2_loss: 58305.5469 - probe_3_loss: 25620.0898 - probe_accuracy: 0.8067\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 124.6824 - probe_loss: 5.2627 - probe_1_loss: 53777.8008 - probe_2_loss: 57528.1641 - probe_3_loss: 8113.7500 - probe_accuracy: 0.9113\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 114.8156 - probe_loss: 4.0747 - probe_1_loss: 50128.6523 - probe_2_loss: 57570.2266 - probe_3_loss: 3042.0632 - probe_accuracy: 0.9342\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 112.8298 - probe_loss: 3.8684 - probe_1_loss: 48770.3711 - probe_2_loss: 57293.3945 - probe_3_loss: 2897.7097 - probe_accuracy: 0.9381\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 111.4935 - probe_loss: 3.7842 - probe_1_loss: 47830.1680 - probe_2_loss: 56814.9180 - probe_3_loss: 3064.1204 - probe_accuracy: 0.9402\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 110.9623 - probe_loss: 3.7524 - probe_1_loss: 47590.3711 - probe_2_loss: 56450.4922 - probe_3_loss: 3169.0718 - probe_accuracy: 0.9446\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 109.8575 - probe_loss: 3.7804 - probe_1_loss: 46855.2188 - probe_2_loss: 56029.6094 - probe_3_loss: 3192.2871 - probe_accuracy: 0.9425\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 108.9246 - probe_loss: 3.6471 - probe_1_loss: 46258.0430 - probe_2_loss: 55775.5039 - probe_3_loss: 3243.8345 - probe_accuracy: 0.9456\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 109.3553 - probe_loss: 3.7684 - probe_1_loss: 46497.7500 - probe_2_loss: 55446.5273 - probe_3_loss: 3642.5161 - probe_accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 108.9646 - probe_loss: 3.6837 - probe_1_loss: 46474.2031 - probe_2_loss: 55290.8867 - probe_3_loss: 3515.7546 - probe_accuracy: 0.9465\n"
     ]
    }
   ],
   "source": [
    "# With FR regularization, you need to train for more epochs.\n",
    "params_file, epochs, target_rate = \"./attempting_TN_MP_loihineurons_8_16\", 10, 250 \n",
    "\n",
    "converter, lyr_probes = _get_nengo_dl_model(\n",
    "  #swap_activations={tf.nn.relu: nengo_loihi.neurons.SpikingRectifiedLinear()}, # With this neuron, spike amplitude\n",
    "                                                                                # is more than 1.\n",
    "  swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}, # Spike amplitude is just 1.\n",
    "  # With both, SpikingRectifiedLinear and LoihiSpikingRectifiedLinear, the network is able to reach just 96% acc.\n",
    "  # when target is set to 250Hz or 150Hz.\n",
    "  \n",
    "  # scale_firing_rates=100 # Without scale_firing_rates, the network learns faster.\n",
    ")\n",
    "  \n",
    "with nengo_dl.Simulator(converter.net, seed=0, minibatch_size=200) as sim:\n",
    "    losses = {\n",
    "      converter.outputs[dense1]: tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      converter.outputs[dense1]: \"accuracy\"\n",
    "    }\n",
    "    \n",
    "    for lyr_probe in lyr_probes:\n",
    "      losses[lyr_probe] = tf.losses.mse\n",
    "      \n",
    "    sim.compile(\n",
    "      loss=losses,\n",
    "      # optimizer=tf.optimizers.Adam(0.001),\n",
    "      optimizer=tf.optimizers.RMSprop(0.001),\n",
    "      #metrics={converter.outputs[dense1]: tf.metrics.sparse_categorical_accuracy},\n",
    "      metrics=metrics,\n",
    "      # Loss Weights important, else network doesn't learn.\n",
    "      # Setting the loss weight of converter.outputs[dense1] to 10, increases the learning, \n",
    "      # but it still limits at 98% acc.\n",
    "      loss_weights={converter.outputs[dense1]: 1, lyr_probes[0]: 1e-3, lyr_probes[1]: 1e-3, lyr_probes[2]: 1e-3},\n",
    "    )\n",
    "    sim.fit(\n",
    "      {converter.inputs[inp]: train_images},\n",
    "      {\n",
    "        converter.outputs[dense1]: train_labels,\n",
    "        \n",
    "        # Following are important for the network to regularize FR. Else, \n",
    "        # probe_1_loss: 0.0000e+00 - probe_2_loss: 0.0000e+00 - probe_3_loss: 0.0000e+00\n",
    "        # is the network output in each epoch.\n",
    "        lyr_probes[0]: np.ones((train_labels.shape[0], 1, lyr_probes[0].size_in)) * target_rate,\n",
    "        lyr_probes[1]: np.ones((train_labels.shape[0], 1, lyr_probes[1].size_in)) * target_rate,\n",
    "        lyr_probes[2]: np.ones((train_labels.shape[0], 1, lyr_probes[2].size_in)) * target_rate,\n",
    "      },\n",
    "      epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # save the parameters to file.\n",
    "    sim.save_params(params_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-german",
   "metadata": {},
   "source": [
    "# Print the `gain`, `bias`, `intercepts`, and `max_rates` of the `Dense0` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acting-powell",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(sim.data[converter.layers[dense0].ensemble].gain)\n",
    "print(sim.data[converter.layers[dense0].ensemble].bias)\n",
    "print(sim.data[converter.layers[dense0].ensemble].intercepts)\n",
    "print(sim.data[converter.layers[dense0].ensemble].max_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-dream",
   "metadata": {},
   "source": [
    "# Test the NengoDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alleged-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nengo DL Test Acc: 80.0\n"
     ]
    }
   ],
   "source": [
    "n_steps=30\n",
    "synapse=0.01\n",
    "n_test=100\n",
    "sfr=1\n",
    "\n",
    "nengo_converter, layer_probes = _get_nengo_dl_model(\n",
    "    scale_firing_rates=sfr, \n",
    "    swap_activations={tf.nn.relu: nengo_loihi.neurons.SpikingRectifiedLinear()}, # With this neuron, spike \n",
    "                                                                                  # amplitude is more than 1.\n",
    "    #swap_activations={tf.nn.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}, # Spike Amplitude is just 1.\n",
    "    synapse=synapse\n",
    ")\n",
    "  \n",
    "tiled_test_images = np.tile(test_images[:n_test], (1, n_steps, 1))\n",
    "# Speed up simulation.\n",
    "with nengo_converter.net:\n",
    "  nengo_dl.configure_settings(stateful=False)\n",
    "    \n",
    "# Build network, load in trained weights, do inference.\n",
    "with nengo_dl.Simulator(\n",
    "    nengo_converter.net, minibatch_size=20, progress_bar=False) as sim:\n",
    "  sim.load_params(params_file)\n",
    "  data = sim.predict({nengo_converter.inputs[inp]: tiled_test_images})\n",
    "    \n",
    "test_predictions = np.argmax(data[nengo_converter.outputs[dense1]][:, -1], axis=-1)\n",
    "print(\"Nengo DL Test Acc: %s\" % (100 * np.mean(test_predictions == test_labels[:n_test, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-indicator",
   "metadata": {},
   "source": [
    "No matter which neuron you use while training: `relu`, or `nengo.RectifedLinear()`, or `nengo_loihi.neurons.LoihiSpikingRectifiedLinear()`, in the absence of `scale_firing_rates`, the network performs very poorly (9%, 17% etc.) when `target_rate` is set to 1.0; in case of FR regularization, the `scale_firing_rates` should ideally be one. \n",
    "\n",
    "When `target_rate` is set to 250 for FR regularization and the `scale_firing_rates` is set to 1, the test accuracy is far better than the above case (e.g. 80%) when the training neuron is `nengo_loihi.neurons.LoihiSpikingRectifiedLinear()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-engine",
   "metadata": {},
   "source": [
    "# Plotting the spikes in Conv0 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "written-amazon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(16,8))\n",
    "rasterplot(np.arange(0, n_steps), data[layer_probes[0]][0, :, np.random.choice(5408, 512, replace=False)].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-virus",
   "metadata": {},
   "source": [
    "# Print the spikes in `Conv0` layer (only if their amplitude is more than 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fabulous-hughes",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv0_spikes_matrix = data[layer_probes[0]] * sfr * 0.001\n",
    "test_image_index = 0\n",
    "\n",
    "for neuron in range(data[layer_probes[0]].shape[2]):\n",
    "  if np.any(conv0_spikes_matrix[test_image_index, :, neuron]):\n",
    "    spikes=np.round(conv0_spikes_matrix[test_image_index, :, neuron])\n",
    "    if set(np.unique(spikes)) - set([0, 1]):\n",
    "      print(spikes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
