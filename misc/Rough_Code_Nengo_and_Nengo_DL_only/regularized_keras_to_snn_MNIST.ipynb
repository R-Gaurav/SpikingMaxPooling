{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fewer-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import nengo_dl\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deluxe-twenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_images, train_labels), (\n",
    "    test_images,\n",
    "    test_labels,\n",
    ") = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# flatten images and add time dimension\n",
    "train_images = train_images.reshape((train_images.shape[0], 1, -1))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1, -1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 1, -1))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], 1, -1))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(np.reshape(train_images[i], (28, 28)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(str(train_labels[i, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "structured-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "inp = tf.keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# convolutional layers\n",
    "conv0 = tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation=tf.nn.relu,\n",
    ")(inp)\n",
    "\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    strides=2,\n",
    "    activation=tf.nn.relu,\n",
    ")(conv0)\n",
    "\n",
    "# fully connected layer\n",
    "flatten = tf.keras.layers.Flatten()(conv1)\n",
    "dense = tf.keras.layers.Dense(units=10)(flatten)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "commercial-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = nengo_dl.Converter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "saved-management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.5585 - probe_loss: 0.5585 - probe_sparse_categorical_accuracy: 0.9244 - val_loss: 0.0760 - val_probe_loss: 0.0760 - val_probe_sparse_categorical_accuracy: 0.9756\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 6s 21ms/step - loss: 0.0595 - probe_loss: 0.0595 - probe_sparse_categorical_accuracy: 0.9822 - val_loss: 0.0637 - val_probe_loss: 0.0637 - val_probe_sparse_categorical_accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "do_training = True\n",
    "if do_training:\n",
    "    with nengo_dl.Simulator(converter.net, minibatch_size=200) as sim:\n",
    "        # run training\n",
    "        sim.compile(\n",
    "            optimizer=tf.optimizers.Adam(0.001),\n",
    "            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[tf.metrics.sparse_categorical_accuracy],\n",
    "        )\n",
    "        sim.fit(\n",
    "            {converter.inputs[inp]: train_images},\n",
    "            {converter.outputs[dense]: train_labels},\n",
    "            validation_data=(\n",
    "                {converter.inputs[inp]: test_images},\n",
    "                {converter.outputs[dense]: test_labels},\n",
    "            ),\n",
    "            epochs=2,\n",
    "        )\n",
    "\n",
    "        # save the parameters to file\n",
    "        sim.save_params(\"./keras_to_snn_params\")\n",
    "else:\n",
    "    # download pretrained weights\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&\"\n",
    "        \"id=1lBkR968AQo__t8sMMeDYGTQpBJZIs2_T\",\n",
    "        \"keras_to_snn_params.npz\",\n",
    "    )\n",
    "    print(\"Loaded pretrained weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "small-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(\n",
    "    activation,\n",
    "    params_file=\"keras_to_snn_params\",\n",
    "    n_steps=30,\n",
    "    scale_firing_rates=1,\n",
    "    synapse=None,\n",
    "    n_test=400,\n",
    "):\n",
    "    # convert the keras model to a nengo network\n",
    "    nengo_converter = nengo_dl.Converter(\n",
    "        model,\n",
    "        swap_activations={tf.nn.relu: activation},\n",
    "        scale_firing_rates=scale_firing_rates,\n",
    "        synapse=synapse,\n",
    "    )\n",
    "\n",
    "    # get input/output objects\n",
    "    nengo_input = nengo_converter.inputs[inp]\n",
    "    nengo_output = nengo_converter.outputs[dense]\n",
    "\n",
    "    # add a probe to the first convolutional layer to record activity.\n",
    "    # we'll only record from a subset of neurons, to save memory.\n",
    "    sample_neurons = np.linspace(\n",
    "        0,\n",
    "        np.prod(conv0.shape[1:]),\n",
    "        1000,\n",
    "        endpoint=False,\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "    with nengo_converter.net:\n",
    "        conv0_probe = nengo.Probe(nengo_converter.layers[conv0][sample_neurons])\n",
    "\n",
    "    # repeat inputs for some number of timesteps\n",
    "    tiled_test_images = np.tile(test_images[:n_test], (1, n_steps, 1))\n",
    "\n",
    "    # set some options to speed up simulation\n",
    "    with nengo_converter.net:\n",
    "        nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    # build network, load in trained weights, run inference on test images\n",
    "    with nengo_dl.Simulator(\n",
    "        nengo_converter.net, minibatch_size=10, progress_bar=False\n",
    "    ) as nengo_sim:\n",
    "        nengo_sim.load_params(params_file)\n",
    "        data = nengo_sim.predict({nengo_input: tiled_test_images})\n",
    "\n",
    "    # compute accuracy on test data, using output of network on\n",
    "    # last timestep\n",
    "    predictions = np.argmax(data[nengo_output][:, -1], axis=-1)\n",
    "    accuracy = (predictions == test_labels[:n_test, 0, 0]).mean()\n",
    "    print(f\"Test accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "#     # plot the results\n",
    "#     for ii in range(3):\n",
    "#         plt.figure(figsize=(12, 4))\n",
    "\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.title(\"Input image\")\n",
    "#         plt.imshow(test_images[ii, 0].reshape((28, 28)), cmap=\"gray\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         scaled_data = data[conv0_probe][ii] * scale_firing_rates\n",
    "#         if isinstance(activation, nengo.SpikingRectifiedLinear):\n",
    "#             scaled_data *= 0.001\n",
    "#             rates = np.sum(scaled_data, axis=0) / (n_steps * nengo_sim.dt)\n",
    "#             plt.ylabel(\"Number of spikes\")\n",
    "#         else:\n",
    "#             rates = scaled_data\n",
    "#             plt.ylabel(\"Firing rates (Hz)\")\n",
    "#         plt.xlabel(\"Timestep\")\n",
    "#         plt.title(\n",
    "#             f\"Neural activities (conv0 mean={rates.mean():.1f} Hz, \"\n",
    "#             f\"max={rates.max():.1f} Hz)\"\n",
    "#         )\n",
    "#         plt.plot(scaled_data)\n",
    "\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.title(\"Output predictions\")\n",
    "#         plt.plot(tf.nn.softmax(data[nengo_output][ii]))\n",
    "#         plt.legend([str(j) for j in range(10)], loc=\"upper left\")\n",
    "#         plt.xlabel(\"Timestep\")\n",
    "#         plt.ylabel(\"Probability\")\n",
    "\n",
    "#         plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "following-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.25%\n"
     ]
    }
   ],
   "source": [
    "run_network(activation=nengo.RectifiedLinear(), n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "experienced-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 23.25%\n"
     ]
    }
   ],
   "source": [
    "run_network(activation=nengo.SpikingRectifiedLinear())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "injured-explosion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synapse=0.001\n",
      "Test accuracy: 25.25%\n",
      "Synapse=0.005\n",
      "Test accuracy: 50.00%\n",
      "Synapse=0.010\n",
      "Test accuracy: 63.50%\n"
     ]
    }
   ],
   "source": [
    "for s in [0.001, 0.005, 0.01]:\n",
    "    print(f\"Synapse={s:.3f}\")\n",
    "    run_network(\n",
    "        activation=nengo.SpikingRectifiedLinear(),\n",
    "        n_steps=60,\n",
    "        synapse=s,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "integrated-edinburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale=2\n",
      "Test accuracy: 80.00%\n",
      "Scale=5\n",
      "Test accuracy: 93.25%\n",
      "Scale=10\n",
      "Test accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "for scale in [2, 5, 10]:\n",
    "    print(f\"Scale={scale}\")\n",
    "    run_network(\n",
    "        activation=nengo.SpikingRectifiedLinear(),\n",
    "        scale_firing_rates=scale,\n",
    "        synapse=0.01,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liquid-swedish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/simulator.py:1773: UserWarning: Number of elements (1) in ['str'] does not match number of Probes (3); consider using an explicit input dictionary in this case, so that the assignment of data to objects is unambiguous.\n",
      "  len(objects),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 116.1265 - probe_loss: 1.9438 - probe_1_loss: 52797.1250 - probe_2_loss: 61385.6055 - probe_accuracy: 0.9050\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 110.0154 - probe_loss: 3.5781 - probe_1_loss: 47302.4219 - probe_2_loss: 59134.8320 - probe_accuracy: 0.9536\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 108.9177 - probe_loss: 3.8814 - probe_1_loss: 46837.9102 - probe_2_loss: 58198.4375 - probe_accuracy: 0.9632\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 108.4366 - probe_loss: 4.4498 - probe_1_loss: 46590.3477 - probe_2_loss: 57396.5000 - probe_accuracy: 0.9660\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 107.9354 - probe_loss: 4.6523 - probe_1_loss: 46416.6328 - probe_2_loss: 56866.4922 - probe_accuracy: 0.9689\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 107.5927 - probe_loss: 5.0439 - probe_1_loss: 46264.0039 - probe_2_loss: 56284.8086 - probe_accuracy: 0.9712\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 106.9532 - probe_loss: 4.9063 - probe_1_loss: 46123.9844 - probe_2_loss: 55923.0000 - probe_accuracy: 0.9729\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 8s 28ms/step - loss: 106.6983 - probe_loss: 5.1979 - probe_1_loss: 46020.1797 - probe_2_loss: 55480.2422 - probe_accuracy: 0.9734\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 8s 28ms/step - loss: 106.2016 - probe_loss: 4.9660 - probe_1_loss: 45936.1094 - probe_2_loss: 55299.4141 - probe_accuracy: 0.9751\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 105.7716 - probe_loss: 5.1662 - probe_1_loss: 45831.3633 - probe_2_loss: 54774.0156 - probe_accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "# we'll encourage the neurons to spike at around 250Hz\n",
    "target_rate = 250\n",
    "\n",
    "# convert keras model to nengo network\n",
    "converter = nengo_dl.Converter(model)\n",
    "\n",
    "# add probes to the convolutional layers, which\n",
    "# we'll use to apply the firing rate regularization\n",
    "with converter.net:\n",
    "    output_p = converter.outputs[dense]\n",
    "    conv0_p = nengo.Probe(converter.layers[conv0])\n",
    "    conv1_p = nengo.Probe(converter.layers[conv1])\n",
    "\n",
    "with nengo_dl.Simulator(converter.net, minibatch_size=200) as sim:\n",
    "    # add regularization loss functions to the convolutional layers\n",
    "    sim.compile(\n",
    "        optimizer=tf.optimizers.RMSprop(0.001),\n",
    "        loss={\n",
    "            output_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            conv0_p: tf.losses.mse,\n",
    "            conv1_p: tf.losses.mse,\n",
    "        },\n",
    "        loss_weights={output_p: 1, conv0_p: 1e-3, conv1_p: 1e-3},\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    do_training = True\n",
    "    if do_training:\n",
    "        # run training (specifying the target rates for the convolutional layers)\n",
    "        sim.fit(\n",
    "            {converter.inputs[inp]: train_images},\n",
    "            {\n",
    "                output_p: train_labels,\n",
    "                conv0_p: np.ones((train_labels.shape[0], 1, conv0_p.size_in))\n",
    "                * target_rate,\n",
    "                conv1_p: np.ones((train_labels.shape[0], 1, conv1_p.size_in))\n",
    "                * target_rate,\n",
    "            },\n",
    "            epochs=10,\n",
    "        )\n",
    "\n",
    "        # save the parameters to file\n",
    "        sim.save_params(\"./keras_to_snn_regularized_params\")\n",
    "    else:\n",
    "        # download pretrained weights\n",
    "        urlretrieve(\n",
    "            \"https://drive.google.com/uc?export=download&\"\n",
    "            \"id=1xvIIIQjiA4UM9Mg_4rq_ttBH3wIl0lJx\",\n",
    "            \"keras_to_snn_regularized_params.npz\",\n",
    "        )\n",
    "        print(\"Loaded pretrained weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "municipal-budget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "run_network(\n",
    "    activation=nengo.RectifiedLinear(),\n",
    "    params_file=\"keras_to_snn_regularized_params\",\n",
    "    n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dress-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.50%\n"
     ]
    }
   ],
   "source": [
    "run_network(\n",
    "    activation=nengo.SpikingRectifiedLinear(),\n",
    "    params_file=\"keras_to_snn_regularized_params\",\n",
    "    synapse=0.005,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "banned-artwork",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "import nengo_loihi\n",
    "\n",
    "run_network(\n",
    "    activation=nengo_loihi.neurons.LoihiSpikingRectifiedLinear(),\n",
    "    params_file=\"keras_to_snn_regularized_params\",\n",
    "    synapse=0.005,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-beauty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
