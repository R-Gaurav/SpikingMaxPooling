{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hungry-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nengo_dl\n",
    "import nengo_loihi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-college",
   "metadata": {},
   "source": [
    "# TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "magnetic-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "channels = \"channels_last\"\n",
    "inp = tf.keras.Input((32, 32, 3))\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(inp)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPool2D((2, 2), data_format=channels)(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPool2D((2, 2), data_format=channels)(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.MaxPool2D((2, 2), data_format=\"channels_last\")(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "x = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bottom-killer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 64)        18432     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        36864     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 128)         73728     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 128)         147456    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 304,362\n",
      "Trainable params: 304,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stunning-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "(50000, 1, 3072) (10000, 1, 3072)\n",
      "(50000, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
    "train_x = train_x.astype(np.float32) / 127.5 - 1\n",
    "test_x = test_x.astype(np.float32) / 127.5 - 1\n",
    "\n",
    "train_y = np.eye(10, dtype=np.float32)[train_y].squeeze(axis=1)\n",
    "test_y = np.eye(10, dtype=np.float32)[test_y].squeeze(axis=1)\n",
    "\n",
    "# train_x, test_x = np.moveaxis(train_x, -1, 1), np.moveaxis(test_x, -1, 1) # Make Channel's Last\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "# Flatten the images.\n",
    "train_x = train_x.reshape(train_x.shape[0], 1, -1)\n",
    "test_x = test_x.reshape(test_x.shape[0], 1, -1)\n",
    "print(train_x.shape, test_x.shape)\n",
    "train_y = train_y.reshape(train_y.shape[0], 1, -1)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-meditation",
   "metadata": {},
   "source": [
    "# Training NengoDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "removed-steam",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Cannot convert max pooling layers to native Nengo objects; consider setting max_to_avg_pool=True to use average pooling instead. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Layer type <class 'tensorflow.python.keras.layers.core.Dropout'> does not have a registered converter. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "ndl_model = nengo_dl.Converter(model, swap_activations=\n",
    "                               {\n",
    "                                 tf.keras.activations.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()\n",
    "                               },\n",
    "                                scale_firing_rates=100\n",
    "                              )\n",
    "ndl_input = ndl_model.inputs[inp]\n",
    "ndl_output = ndl_model.outputs[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sunrise-glenn",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 1.7151 - probe_loss: 1.7151 - probe_accuracy: 0.3637\n",
      "Epoch 2/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 1.3072 - probe_loss: 1.3072 - probe_accuracy: 0.5339\n",
      "Epoch 3/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 1.1090 - probe_loss: 1.1090 - probe_accuracy: 0.6106\n",
      "Epoch 4/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.9919 - probe_loss: 0.9919 - probe_accuracy: 0.6577\n",
      "Epoch 5/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.9058 - probe_loss: 0.9058 - probe_accuracy: 0.6876\n",
      "Epoch 6/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.8544 - probe_loss: 0.8544 - probe_accuracy: 0.7059\n",
      "Epoch 7/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.7972 - probe_loss: 0.7972 - probe_accuracy: 0.7297\n",
      "Epoch 8/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.7598 - probe_loss: 0.7598 - probe_accuracy: 0.7404\n",
      "Epoch 9/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.7256 - probe_loss: 0.7256 - probe_accuracy: 0.7517\n",
      "Epoch 10/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6988 - probe_loss: 0.6988 - probe_accuracy: 0.7613\n",
      "Epoch 11/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6641 - probe_loss: 0.6641 - probe_accuracy: 0.7744\n",
      "Epoch 12/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6447 - probe_loss: 0.6447 - probe_accuracy: 0.7795\n",
      "Epoch 13/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6214 - probe_loss: 0.6214 - probe_accuracy: 0.7868\n",
      "Epoch 14/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6017 - probe_loss: 0.6017 - probe_accuracy: 0.7941\n",
      "Epoch 15/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5872 - probe_loss: 0.5872 - probe_accuracy: 0.7989\n",
      "Epoch 16/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5720 - probe_loss: 0.5720 - probe_accuracy: 0.8060\n",
      "Epoch 17/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5591 - probe_loss: 0.5591 - probe_accuracy: 0.8101\n",
      "Epoch 18/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5381 - probe_loss: 0.5381 - probe_accuracy: 0.8138\n",
      "Epoch 19/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5231 - probe_loss: 0.5231 - probe_accuracy: 0.8209\n",
      "Epoch 20/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5143 - probe_loss: 0.5143 - probe_accuracy: 0.8240\n",
      "Epoch 21/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4963 - probe_loss: 0.4963 - probe_accuracy: 0.8288\n",
      "Epoch 22/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4915 - probe_loss: 0.4915 - probe_accuracy: 0.8318\n",
      "Epoch 23/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4749 - probe_loss: 0.4749 - probe_accuracy: 0.8351\n",
      "Epoch 24/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4690 - probe_loss: 0.4690 - probe_accuracy: 0.8385\n",
      "Epoch 25/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4543 - probe_loss: 0.4543 - probe_accuracy: 0.8422\n",
      "Epoch 26/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4462 - probe_loss: 0.4462 - probe_accuracy: 0.8447\n",
      "Epoch 27/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4353 - probe_loss: 0.4353 - probe_accuracy: 0.8498\n",
      "Epoch 28/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4242 - probe_loss: 0.4242 - probe_accuracy: 0.8525\n",
      "Epoch 29/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4286 - probe_loss: 0.4286 - probe_accuracy: 0.8511\n",
      "Epoch 30/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4186 - probe_loss: 0.4186 - probe_accuracy: 0.8564\n",
      "Epoch 31/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4064 - probe_loss: 0.4064 - probe_accuracy: 0.8589\n",
      "Epoch 32/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4068 - probe_loss: 0.4068 - probe_accuracy: 0.8592\n",
      "Epoch 33/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3966 - probe_loss: 0.3966 - probe_accuracy: 0.8636\n",
      "Epoch 34/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3934 - probe_loss: 0.3934 - probe_accuracy: 0.8644\n",
      "Epoch 35/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3862 - probe_loss: 0.3862 - probe_accuracy: 0.8666\n",
      "Epoch 36/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3813 - probe_loss: 0.3813 - probe_accuracy: 0.8671\n",
      "Epoch 37/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3730 - probe_loss: 0.3730 - probe_accuracy: 0.8716\n",
      "Epoch 38/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3707 - probe_loss: 0.3707 - probe_accuracy: 0.8712\n",
      "Epoch 39/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3630 - probe_loss: 0.3630 - probe_accuracy: 0.8739\n",
      "Epoch 40/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3654 - probe_loss: 0.3654 - probe_accuracy: 0.8732\n",
      "Epoch 41/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3518 - probe_loss: 0.3518 - probe_accuracy: 0.8779\n",
      "Epoch 42/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3452 - probe_loss: 0.3452 - probe_accuracy: 0.8796\n",
      "Epoch 43/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3459 - probe_loss: 0.3459 - probe_accuracy: 0.8768\n",
      "Epoch 44/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3426 - probe_loss: 0.3426 - probe_accuracy: 0.8807\n",
      "Epoch 45/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3341 - probe_loss: 0.3341 - probe_accuracy: 0.8825\n",
      "Epoch 46/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3344 - probe_loss: 0.3344 - probe_accuracy: 0.8833\n",
      "Epoch 47/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3308 - probe_loss: 0.3308 - probe_accuracy: 0.8845\n",
      "Epoch 48/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3294 - probe_loss: 0.3294 - probe_accuracy: 0.8855\n",
      "Epoch 49/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3278 - probe_loss: 0.3278 - probe_accuracy: 0.8851\n",
      "Epoch 50/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3177 - probe_loss: 0.3177 - probe_accuracy: 0.8895\n",
      "Epoch 51/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3132 - probe_loss: 0.3132 - probe_accuracy: 0.8893\n",
      "Epoch 52/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3138 - probe_loss: 0.3138 - probe_accuracy: 0.8903\n",
      "Epoch 53/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3104 - probe_loss: 0.3104 - probe_accuracy: 0.8920\n",
      "Epoch 54/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3071 - probe_loss: 0.3071 - probe_accuracy: 0.8917\n",
      "Epoch 55/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3003 - probe_loss: 0.3003 - probe_accuracy: 0.8941\n",
      "Epoch 56/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3051 - probe_loss: 0.3051 - probe_accuracy: 0.8941\n",
      "Epoch 57/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2994 - probe_loss: 0.2994 - probe_accuracy: 0.8962\n",
      "Epoch 58/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2994 - probe_loss: 0.2994 - probe_accuracy: 0.8958\n",
      "Epoch 59/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2969 - probe_loss: 0.2969 - probe_accuracy: 0.8977\n",
      "Epoch 60/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2905 - probe_loss: 0.2905 - probe_accuracy: 0.8985\n",
      "Epoch 61/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2919 - probe_loss: 0.2919 - probe_accuracy: 0.8977\n",
      "Epoch 62/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2870 - probe_loss: 0.2870 - probe_accuracy: 0.8992\n",
      "Epoch 63/64\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2799 - probe_loss: 0.2799 - probe_accuracy: 0.9027\n",
      "Epoch 64/64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2774 - probe_loss: 0.2774 - probe_accuracy: 0.9027\n"
     ]
    }
   ],
   "source": [
    "with nengo_dl.Simulator(ndl_model.net, minibatch_size=100, seed=0, progress_bar=False) as ndl_sim:\n",
    "  losses  = {\n",
    "    ndl_output: tf.losses.CategoricalCrossentropy(from_logits=True, )\n",
    "  }\n",
    "  ndl_sim.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=1e-3, decay=1e-4),\n",
    "    loss=losses,\n",
    "    metrics=[\"accuracy\"]\n",
    "  )\n",
    "  ndl_sim.fit(\n",
    "    {ndl_input: train_x, \n",
    "#      \"n_steps\": np.ones((train_x.shape[0], 1), dtype=np.int32), \n",
    "#      \"dense_1.0.bias\": np.ones((train_x.shape[0], 10, 1), dtype=np.int32)\n",
    "    },\n",
    "    {ndl_output: train_y},\n",
    "    epochs=64,\n",
    "  )\n",
    "  ndl_sim.save_params(\"./ndl_trained_params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-dealer",
   "metadata": {},
   "source": [
    "# Testing NengoDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "looking-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 64)        18432     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        36864     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 128)         73728     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 128)         147456    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 304,362\n",
      "Trainable params: 304,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [l for l in model.layers]\n",
    "x0 = layers[0].output\n",
    "for i in range(1, len(layers)):\n",
    "  if layers[i].name.startswith(\"dropout\"):\n",
    "    continue\n",
    "  x0 = layers[i](x0)\n",
    "\n",
    "new_model = tf.keras.Model(inputs=layers[0].input, outputs=x0)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legislative-beauty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Cannot convert max pooling layers to native Nengo objects; consider setting max_to_avg_pool=True to use average pooling instead. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n"
     ]
    }
   ],
   "source": [
    "test_x_tiled = np.tile(test_x, (1, 100, 1))\n",
    "ndl_model_test = nengo_dl.Converter(\n",
    "    new_model,\n",
    "    scale_firing_rates=400,\n",
    "    synapse=0.005,\n",
    "    swap_activations={tf.keras.activations.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()}\n",
    "    )\n",
    "ndl_input = ndl_model_test.inputs[inp]\n",
    "ndl_output = ndl_model_test.outputs[x0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focused-front",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/simulator.py:1773: UserWarning: Number of elements (1) in ['ndarray'] does not match number of Nodes (3); consider using an explicit input dictionary in this case, so that the assignment of data to objects is unambiguous.\n",
      "  len(objects),\n"
     ]
    }
   ],
   "source": [
    "with nengo_dl.Simulator(\n",
    "    ndl_model_test.net, minibatch_size=100, progress_bar=False) as ndl_sim_test:\n",
    "  ndl_sim_test.load_params(\"./ndl_trained_params\")\n",
    "  sim_data = ndl_sim_test.predict(test_x_tiled[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "choice-double",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data[ndl_output].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "congressional-championship",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_clss = np.argmax(sim_data[ndl_output][:, -1, :], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "speaking-denmark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.71000000000001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*np.mean(pred_clss == np.argmax(test_y[:], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hairy-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<Reference wrapping <tf.Tensor 'input_1:0' shape=(None, 32, 32, 3) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_6/Identity:0' shape=(None, 30, 30, 32) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_1_1/Identity:0' shape=(None, 28, 28, 32) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'max_pooling2d_2/Identity:0' shape=(None, 14, 14, 32) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_2_1/Identity:0' shape=(None, 12, 12, 64) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_3_1/Identity:0' shape=(None, 10, 10, 64) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'max_pooling2d_1_1/Identity:0' shape=(None, 5, 5, 64) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_4_1/Identity:0' shape=(None, 3, 3, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_5_1/Identity:0' shape=(None, 1, 1, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'flatten_1/Identity:0' shape=(None, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'dense_2/Identity:0' shape=(None, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'dense_1_1/Identity:0' shape=(None, 10) dtype=float32>>,)\n"
     ]
    }
   ],
   "source": [
    "for lyr in ndl_model_test.net.layers:\n",
    "  print(lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "polish-taiwan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<Reference wrapping <tf.Tensor 'input_1:0' shape=(None, 32, 32, 3) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_6/Identity:0' shape=(None, 30, 30, 32) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_1_1/Identity:0' shape=(None, 28, 28, 32) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'max_pooling2d_2/Identity:0' shape=(None, 14, 14, 32) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_2_1/Identity:0' shape=(None, 12, 12, 64) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_3_1/Identity:0' shape=(None, 10, 10, 64) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'max_pooling2d_1_1/Identity:0' shape=(None, 5, 5, 64) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_4_1/Identity:0' shape=(None, 3, 3, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'conv2d_5_1/Identity:0' shape=(None, 1, 1, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'flatten_1/Identity:0' shape=(None, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'dense_2/Identity:0' shape=(None, 128) dtype=float32>>,)\n",
      "(<Reference wrapping <tf.Tensor 'dense_1_1/Identity:0' shape=(None, 10) dtype=float32>>,)\n"
     ]
    }
   ],
   "source": [
    "for lyr in ndl_model_test.net.layers:\n",
    "  print(lyr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
