{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "import nengo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "channels = \"channels_last\"\n",
    "inp = tf.keras.Input((32, 32, 3))\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(inp)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.MaxPool2D((2, 2), data_format=channels)(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.MaxPool2D((2, 2), data_format=channels)(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), data_format=channels, activation=\"relu\", use_bias=False)(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.ReLU()(x)\n",
    "# x = tf.keras.layers.MaxPool2D((2, 2), data_format=\"channels_last\")(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 64)        18432     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        36864     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 128)         73728     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 128)         147456    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 304,362\n",
      "Trainable params: 304,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "(50000, 1, 3072) (10000, 1, 3072)\n",
      "(50000, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
    "train_x = train_x.astype(np.float32) / 127.5 - 1\n",
    "test_x = test_x.astype(np.float32) / 127.5 - 1\n",
    "\n",
    "train_y = np.eye(10, dtype=np.float32)[train_y].squeeze(axis=1)\n",
    "test_y = np.eye(10, dtype=np.float32)[test_y].squeeze(axis=1)\n",
    "\n",
    "# train_x, test_x = np.moveaxis(train_x, -1, 1), np.moveaxis(test_x, -1, 1) # Make Channel's Last\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "# Flatten the images.\n",
    "train_x = train_x.reshape(train_x.shape[0], 1, -1)\n",
    "test_x = test_x.reshape(test_x.shape[0], 1, -1)\n",
    "print(train_x.shape, test_x.shape)\n",
    "train_y = train_y.reshape(train_y.shape[0], 1, -1)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NengoDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Cannot convert max pooling layers to native Nengo objects; consider setting max_to_avg_pool=True to use average pooling instead. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "/home/rgaurav/miniconda3/envs/nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Layer type <class 'tensorflow.python.keras.layers.core.Dropout'> does not have a registered converter. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "ndl_model = nengo_dl.Converter(model, swap_activations=\n",
    "                               {\n",
    "                                 tf.keras.activations.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()\n",
    "                               },\n",
    "                                scale_firing_rates=400\n",
    "                              )\n",
    "ndl_input = ndl_model.inputs[inp]\n",
    "ndl_output = ndl_model.outputs[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 1.8451 - probe_loss: 1.8451 - probe_accuracy: 0.3006\n",
      "Epoch 2/128\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.4390 - probe_loss: 1.4390 - probe_accuracy: 0.4819\n",
      "Epoch 3/128\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.2405 - probe_loss: 1.2405 - probe_accuracy: 0.5649\n",
      "Epoch 4/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 1.1188 - probe_loss: 1.1188 - probe_accuracy: 0.6115\n",
      "Epoch 5/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 1.0307 - probe_loss: 1.0307 - probe_accuracy: 0.6424\n",
      "Epoch 6/128\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.9651 - probe_loss: 0.9651 - probe_accuracy: 0.6697\n",
      "Epoch 7/128\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.9055 - probe_loss: 0.9055 - probe_accuracy: 0.6913\n",
      "Epoch 8/128\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.8696 - probe_loss: 0.8696 - probe_accuracy: 0.7059\n",
      "Epoch 9/128\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.8287 - probe_loss: 0.8287 - probe_accuracy: 0.7182\n",
      "Epoch 10/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.8062 - probe_loss: 0.8062 - probe_accuracy: 0.7278\n",
      "Epoch 11/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.7824 - probe_loss: 0.7824 - probe_accuracy: 0.7367\n",
      "Epoch 12/128\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.7458 - probe_loss: 0.7458 - probe_accuracy: 0.7489\n",
      "Epoch 13/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.7329 - probe_loss: 0.7329 - probe_accuracy: 0.7541\n",
      "Epoch 14/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.7189 - probe_loss: 0.7189 - probe_accuracy: 0.7592\n",
      "Epoch 15/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.6974 - probe_loss: 0.6974 - probe_accuracy: 0.7659\n",
      "Epoch 16/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.6831 - probe_loss: 0.6831 - probe_accuracy: 0.7695\n",
      "Epoch 17/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.6745 - probe_loss: 0.6745 - probe_accuracy: 0.7730\n",
      "Epoch 18/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.6563 - probe_loss: 0.6563 - probe_accuracy: 0.7805\n",
      "Epoch 19/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.6448 - probe_loss: 0.6448 - probe_accuracy: 0.7845\n",
      "Epoch 20/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.6361 - probe_loss: 0.6361 - probe_accuracy: 0.7850\n",
      "Epoch 21/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.6235 - probe_loss: 0.6235 - probe_accuracy: 0.7921\n",
      "Epoch 22/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.6216 - probe_loss: 0.6216 - probe_accuracy: 0.7933\n",
      "Epoch 23/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.6143 - probe_loss: 0.6143 - probe_accuracy: 0.7955\n",
      "Epoch 24/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5992 - probe_loss: 0.5992 - probe_accuracy: 0.7993\n",
      "Epoch 25/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5975 - probe_loss: 0.5975 - probe_accuracy: 0.8024\n",
      "Epoch 26/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5927 - probe_loss: 0.5927 - probe_accuracy: 0.8021\n",
      "Epoch 27/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5892 - probe_loss: 0.5892 - probe_accuracy: 0.8036\n",
      "Epoch 28/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5769 - probe_loss: 0.5769 - probe_accuracy: 0.8073\n",
      "Epoch 29/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5734 - probe_loss: 0.5734 - probe_accuracy: 0.8086\n",
      "Epoch 30/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5748 - probe_loss: 0.5748 - probe_accuracy: 0.8079\n",
      "Epoch 31/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.5716 - probe_loss: 0.5716 - probe_accuracy: 0.8086\n",
      "Epoch 32/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5648 - probe_loss: 0.5648 - probe_accuracy: 0.8131\n",
      "Epoch 33/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5625 - probe_loss: 0.5625 - probe_accuracy: 0.8135\n",
      "Epoch 34/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5533 - probe_loss: 0.5533 - probe_accuracy: 0.8156\n",
      "Epoch 35/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5497 - probe_loss: 0.5497 - probe_accuracy: 0.8185\n",
      "Epoch 36/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5509 - probe_loss: 0.5509 - probe_accuracy: 0.8172\n",
      "Epoch 37/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5438 - probe_loss: 0.5438 - probe_accuracy: 0.8186\n",
      "Epoch 38/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5410 - probe_loss: 0.5410 - probe_accuracy: 0.8209\n",
      "Epoch 39/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5369 - probe_loss: 0.5369 - probe_accuracy: 0.8231\n",
      "Epoch 40/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.5369 - probe_loss: 0.5369 - probe_accuracy: 0.8220\n",
      "Epoch 41/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5327 - probe_loss: 0.5327 - probe_accuracy: 0.8216\n",
      "Epoch 42/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5351 - probe_loss: 0.5351 - probe_accuracy: 0.8224\n",
      "Epoch 43/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5267 - probe_loss: 0.5267 - probe_accuracy: 0.8263\n",
      "Epoch 44/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5269 - probe_loss: 0.5269 - probe_accuracy: 0.8244\n",
      "Epoch 45/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5272 - probe_loss: 0.5272 - probe_accuracy: 0.8258\n",
      "Epoch 46/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5178 - probe_loss: 0.5178 - probe_accuracy: 0.8278\n",
      "Epoch 47/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5172 - probe_loss: 0.5172 - probe_accuracy: 0.8295\n",
      "Epoch 48/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5120 - probe_loss: 0.5120 - probe_accuracy: 0.8298\n",
      "Epoch 49/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5163 - probe_loss: 0.5163 - probe_accuracy: 0.8299\n",
      "Epoch 50/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5150 - probe_loss: 0.5150 - probe_accuracy: 0.8276\n",
      "Epoch 51/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5109 - probe_loss: 0.5109 - probe_accuracy: 0.8302\n",
      "Epoch 52/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5044 - probe_loss: 0.5044 - probe_accuracy: 0.8327\n",
      "Epoch 53/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5176 - probe_loss: 0.5176 - probe_accuracy: 0.8289\n",
      "Epoch 54/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.5023 - probe_loss: 0.5023 - probe_accuracy: 0.8339\n",
      "Epoch 55/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.5043 - probe_loss: 0.5043 - probe_accuracy: 0.8342\n",
      "Epoch 56/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4981 - probe_loss: 0.4981 - probe_accuracy: 0.8346\n",
      "Epoch 57/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.4998 - probe_loss: 0.4998 - probe_accuracy: 0.8341\n",
      "Epoch 58/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4986 - probe_loss: 0.4986 - probe_accuracy: 0.8353\n",
      "Epoch 59/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4930 - probe_loss: 0.4930 - probe_accuracy: 0.8375: 2s - lo\n",
      "Epoch 60/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4880 - probe_loss: 0.4880 - probe_accuracy: 0.8378\n",
      "Epoch 61/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4920 - probe_loss: 0.4920 - probe_accuracy: 0.8377\n",
      "Epoch 62/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4890 - probe_loss: 0.4890 - probe_accuracy: 0.8375\n",
      "Epoch 63/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4878 - probe_loss: 0.4878 - probe_accuracy: 0.8372\n",
      "Epoch 64/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4926 - probe_loss: 0.4926 - probe_accuracy: 0.8382\n",
      "Epoch 65/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4858 - probe_loss: 0.4858 - probe_accuracy: 0.8393\n",
      "Epoch 66/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4806 - probe_loss: 0.4806 - probe_accuracy: 0.8400\n",
      "Epoch 67/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4943 - probe_loss: 0.4943 - probe_accuracy: 0.8373\n",
      "Epoch 68/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4774 - probe_loss: 0.4774 - probe_accuracy: 0.8409\n",
      "Epoch 69/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4823 - probe_loss: 0.4823 - probe_accuracy: 0.8412\n",
      "Epoch 70/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4795 - probe_loss: 0.4795 - probe_accuracy: 0.8417\n",
      "Epoch 71/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4773 - probe_loss: 0.4773 - probe_accuracy: 0.8434\n",
      "Epoch 72/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4751 - probe_loss: 0.4751 - probe_accuracy: 0.8420\n",
      "Epoch 73/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4843 - probe_loss: 0.4843 - probe_accuracy: 0.8441\n",
      "Epoch 74/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4760 - probe_loss: 0.4760 - probe_accuracy: 0.8433\n",
      "Epoch 75/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4708 - probe_loss: 0.4708 - probe_accuracy: 0.8431\n",
      "Epoch 76/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4756 - probe_loss: 0.4756 - probe_accuracy: 0.8440\n",
      "Epoch 77/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4755 - probe_loss: 0.4755 - probe_accuracy: 0.8430\n",
      "Epoch 78/128\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.4676 - probe_loss: 0.4676 - probe_accuracy: 0.8464\n",
      "Epoch 79/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4791 - probe_loss: 0.4791 - probe_accuracy: 0.8420\n",
      "Epoch 80/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.4697 - probe_loss: 0.4697 - probe_accuracy: 0.8462\n",
      "Epoch 81/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4666 - probe_loss: 0.4666 - probe_accuracy: 0.8444\n",
      "Epoch 82/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4661 - probe_loss: 0.4661 - probe_accuracy: 0.8455\n",
      "Epoch 83/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4658 - probe_loss: 0.4658 - probe_accuracy: 0.8467\n",
      "Epoch 84/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4678 - probe_loss: 0.4678 - probe_accuracy: 0.8433\n",
      "Epoch 85/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4615 - probe_loss: 0.4615 - probe_accuracy: 0.8498\n",
      "Epoch 86/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4588 - probe_loss: 0.4588 - probe_accuracy: 0.8475\n",
      "Epoch 87/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4573 - probe_loss: 0.4573 - probe_accuracy: 0.8481\n",
      "Epoch 88/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4659 - probe_loss: 0.4659 - probe_accuracy: 0.8465\n",
      "Epoch 89/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4581 - probe_loss: 0.4581 - probe_accuracy: 0.8493\n",
      "Epoch 90/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.4612 - probe_loss: 0.4612 - probe_accuracy: 0.8491\n",
      "Epoch 91/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4588 - probe_loss: 0.4588 - probe_accuracy: 0.8483\n",
      "Epoch 92/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4583 - probe_loss: 0.4583 - probe_accuracy: 0.8486\n",
      "Epoch 93/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4589 - probe_loss: 0.4589 - probe_accuracy: 0.8505\n",
      "Epoch 94/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4571 - probe_loss: 0.4571 - probe_accuracy: 0.8484\n",
      "Epoch 95/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4593 - probe_loss: 0.4593 - probe_accuracy: 0.8492\n",
      "Epoch 96/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4545 - probe_loss: 0.4545 - probe_accuracy: 0.8488\n",
      "Epoch 97/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4530 - probe_loss: 0.4530 - probe_accuracy: 0.8487\n",
      "Epoch 98/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4525 - probe_loss: 0.4525 - probe_accuracy: 0.8511\n",
      "Epoch 99/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4516 - probe_loss: 0.4516 - probe_accuracy: 0.8506\n",
      "Epoch 100/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.4499 - probe_loss: 0.4499 - probe_accuracy: 0.8518\n",
      "Epoch 101/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4536 - probe_loss: 0.4536 - probe_accuracy: 0.8494\n",
      "Epoch 102/128\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.4500 - probe_loss: 0.4500 - probe_accuracy: 0.8524\n",
      "Epoch 103/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4490 - probe_loss: 0.4490 - probe_accuracy: 0.8523\n",
      "Epoch 104/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4411 - probe_loss: 0.4411 - probe_accuracy: 0.8543\n",
      "Epoch 105/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4411 - probe_loss: 0.4411 - probe_accuracy: 0.8561\n",
      "Epoch 106/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4493 - probe_loss: 0.4493 - probe_accuracy: 0.8517\n",
      "Epoch 107/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4402 - probe_loss: 0.4402 - probe_accuracy: 0.8558\n",
      "Epoch 108/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4408 - probe_loss: 0.4408 - probe_accuracy: 0.8546\n",
      "Epoch 109/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4408 - probe_loss: 0.4408 - probe_accuracy: 0.8548\n",
      "Epoch 110/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4324 - probe_loss: 0.4324 - probe_accuracy: 0.8568\n",
      "Epoch 111/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4322 - probe_loss: 0.4322 - probe_accuracy: 0.8564\n",
      "Epoch 112/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4414 - probe_loss: 0.4414 - probe_accuracy: 0.8537\n",
      "Epoch 113/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4387 - probe_loss: 0.4387 - probe_accuracy: 0.8567\n",
      "Epoch 114/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4405 - probe_loss: 0.4405 - probe_accuracy: 0.8537\n",
      "Epoch 115/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4327 - probe_loss: 0.4327 - probe_accuracy: 0.8563\n",
      "Epoch 116/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4372 - probe_loss: 0.4372 - probe_accuracy: 0.8558\n",
      "Epoch 117/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4332 - probe_loss: 0.4332 - probe_accuracy: 0.8546\n",
      "Epoch 118/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4350 - probe_loss: 0.4350 - probe_accuracy: 0.8569\n",
      "Epoch 119/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4324 - probe_loss: 0.4324 - probe_accuracy: 0.8583\n",
      "Epoch 120/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4345 - probe_loss: 0.4345 - probe_accuracy: 0.8569\n",
      "Epoch 121/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4343 - probe_loss: 0.4343 - probe_accuracy: 0.8564\n",
      "Epoch 122/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4386 - probe_loss: 0.4386 - probe_accuracy: 0.8551\n",
      "Epoch 123/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4342 - probe_loss: 0.4342 - probe_accuracy: 0.8579\n",
      "Epoch 124/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4283 - probe_loss: 0.4283 - probe_accuracy: 0.8590\n",
      "Epoch 125/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4362 - probe_loss: 0.4362 - probe_accuracy: 0.8567\n",
      "Epoch 126/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4331 - probe_loss: 0.4331 - probe_accuracy: 0.8563\n",
      "Epoch 127/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4313 - probe_loss: 0.4313 - probe_accuracy: 0.8581\n",
      "Epoch 128/128\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4270 - probe_loss: 0.4270 - probe_accuracy: 0.8598\n"
     ]
    }
   ],
   "source": [
    "with nengo_dl.Simulator(ndl_model.net, minibatch_size=100, seed=0, progress_bar=False) as ndl_sim:\n",
    "  losses  = {\n",
    "    ndl_output: tf.losses.CategoricalCrossentropy(from_logits=True, )\n",
    "  }\n",
    "  ndl_sim.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=1e-3, decay=1e-5),\n",
    "    loss=losses,\n",
    "    metrics=[\"accuracy\"]\n",
    "  )\n",
    "  ndl_sim.fit(\n",
    "    {ndl_input: train_x, \n",
    "#      \"n_steps\": np.ones((train_x.shape[0], 1), dtype=np.int32), \n",
    "#      \"dense_1.0.bias\": np.ones((train_x.shape[0], 10, 1), dtype=np.int32)\n",
    "    },\n",
    "    {ndl_output: train_y},\n",
    "    epochs=128,\n",
    "  )\n",
    "  ndl_sim.save_params(\"./ndl_trained_params_sfr_400_epochs_128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NengoDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 64)        18432     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        36864     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 128)         73728     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 128)         147456    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 304,362\n",
      "Trainable params: 304,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [l for l in model.layers]\n",
    "layer_objs_lst = []\n",
    "x0 = layers[0].output\n",
    "layer_objs_lst.append(x0)\n",
    "for i in range(1, len(layers)):\n",
    "  if layers[i].name.startswith(\"dropout\"):\n",
    "    continue\n",
    "  x0 = layers[i](x0)\n",
    "  layer_objs_lst.append(x0)\n",
    "\n",
    "new_model = tf.keras.Model(inputs=layers[0].input, outputs=x0)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgaurav/miniconda3/envs/nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:326: UserWarning: Cannot convert max pooling layers to native Nengo objects; consider setting max_to_avg_pool=True to use average pooling instead. Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n"
     ]
    }
   ],
   "source": [
    "test_x_tiled = np.tile(test_x, (1, 120, 1))\n",
    "ndl_model_test = nengo_dl.Converter(\n",
    "    new_model,\n",
    "    scale_firing_rates=400,\n",
    "    synapse=0.005,\n",
    "    swap_activations={tf.keras.activations.relu: nengo_loihi.neurons.LoihiSpikingRectifiedLinear()},\n",
    "    )\n",
    "ndl_input = ndl_model_test.inputs[inp]\n",
    "ndl_output = ndl_model_test.outputs[x0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tensor(\"input_1:0\", shape=(None, 32, 32, 3), dtype=float32)\n",
      "1 Tensor(\"conv2d_6/Identity:0\", shape=(None, 30, 30, 32), dtype=float32)\n",
      "2 Tensor(\"conv2d_1_1/Identity:0\", shape=(None, 28, 28, 32), dtype=float32)\n",
      "3 Tensor(\"max_pooling2d_2/Identity:0\", shape=(None, 14, 14, 32), dtype=float32)\n",
      "4 Tensor(\"conv2d_2_1/Identity:0\", shape=(None, 12, 12, 64), dtype=float32)\n",
      "5 Tensor(\"conv2d_3_1/Identity:0\", shape=(None, 10, 10, 64), dtype=float32)\n",
      "6 Tensor(\"max_pooling2d_1_1/Identity:0\", shape=(None, 5, 5, 64), dtype=float32)\n",
      "7 Tensor(\"conv2d_4_1/Identity:0\", shape=(None, 3, 3, 128), dtype=float32)\n",
      "8 Tensor(\"conv2d_5_1/Identity:0\", shape=(None, 1, 1, 128), dtype=float32)\n",
      "9 Tensor(\"flatten_1/Identity:0\", shape=(None, 128), dtype=float32)\n",
      "10 Tensor(\"dense_2/Identity:0\", shape=(None, 128), dtype=float32)\n",
      "11 Tensor(\"dense_1_1/Identity:0\", shape=(None, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#for i, layer in enumerate(new_model.layers):\n",
    "for i, layer in enumerate(layer_objs_lst):\n",
    "    print(i, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output',)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndl_model_test.layers[layer_objs_lst[3]].probeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ndl_model_test.net:\n",
    "#     mp1_probe = nengo.Probe(ndl_model_test.layers[layer_objs_lst[3]])\n",
    "#     mp2_probe = nengo.Probe(ndl_model_test.layers[layer_objs_lst[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(\n",
    "    ndl_model_test.net, minibatch_size=100, progress_bar=False) as ndl_sim_test:\n",
    "  ndl_sim_test.load_params(\"./ndl_trained_params_sfr_400_epochs_128\")#_epochs_64\")\n",
    "  sim_data = ndl_sim_test.predict({ndl_input: test_x_tiled[:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_clss = np.argmax(sim_data[ndl_output][:, -1, :], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.05"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*np.mean(pred_clss == np.argmax(test_y[:], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_data[ndl_output].shape\n",
    "np.save(\"NDL_pred_output_epochs_128_sfr_400\", sim_data[ndl_output])\n",
    "# np.save(\"NDL_pred_output_epochs_128_sfr_400_mp1_actvns\", sim_data[mp1_probe])\n",
    "# np.save(\"NDL_pred_output_epochs_128_sfr_400_mp2_actvns\", sim_data[mp2_probe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in ndl_model_test.net.layers:\n",
    "  print(lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyr in ndl_model_test.net.layers:\n",
    "  print(lyr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
